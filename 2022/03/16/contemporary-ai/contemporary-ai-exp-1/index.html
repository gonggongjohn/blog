

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="GONGGONGJOHN">
  <meta name="keywords" content="">
  
    <meta name="description" content="摘要 文本分类是自然语言处理领域中一项十分基本的任务，其在多个领域都有着广泛的应用。在本文中，我们实现了基于 XLNet 预训练语言模型的文本分类模型，并使用微调(Fine-Tuning)的方法对给定数据集进行了文 本分类。通过对验证集的分类效果评估比较，我们得到了适用于当前数据集上最佳的模型超参数组合。随后，我们将模型与其他几种经典文本分类模型(支持向量机、MLP、TextCNN、Bert)的分">
<meta property="og:type" content="article">
<meta property="og:title" content="当代人工智能项目一 文本分类">
<meta property="og:url" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/index.html">
<meta property="og:site_name" content="GONGGONGJOHN&#39;s Blog">
<meta property="og:description" content="摘要 文本分类是自然语言处理领域中一项十分基本的任务，其在多个领域都有着广泛的应用。在本文中，我们实现了基于 XLNet 预训练语言模型的文本分类模型，并使用微调(Fine-Tuning)的方法对给定数据集进行了文 本分类。通过对验证集的分类效果评估比较，我们得到了适用于当前数据集上最佳的模型超参数组合。随后，我们将模型与其他几种经典文本分类模型(支持向量机、MLP、TextCNN、Bert)的分">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/dataset_wordcloud.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/fcnn_neuron.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/fcnn_structure.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/textcnn_illustrate.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/bert_classify.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/xlnet_architecture.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/batchsize_loss_acc.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/gpu_mem.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/loss_lr.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/scheduler_lr.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/epoch_loss_acc.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/cross_val.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/evaluation_score.png">
<meta property="og:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/models_acc_fold.png">
<meta property="article:published_time" content="2022-03-16T02:00:00.000Z">
<meta property="article:modified_time" content="2022-07-18T09:33:17.236Z">
<meta property="article:author" content="GONGGONGJOHN">
<meta property="article:tag" content="Computer-Science">
<meta property="article:tag" content="Artificial-Intelligence">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://gonggongjohn.me/2022/03/16/contemporary-ai/contemporary-ai-exp-1/dataset_wordcloud.png">
  
  
  <title>当代人工智能项目一 文本分类 - GONGGONGJOHN&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"gonggongjohn.me","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="GONGGONGJOHN's Blog" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>GONGGONGJOHN&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="当代人工智能项目一 文本分类">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-03-16 10:00" pubdate>
        2022年3月16日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      14k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      117 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">当代人工智能项目一 文本分类</h1>
            
            <div class="markdown-body">
              <h2 id="摘要">摘要</h2>
<p>文本分类是自然语言处理领域中一项十分基本的任务，其在多个领域都有着广泛的应用。在本文中，我们实现了基于 XLNet 预训练语言模型的文本分类模型，并使用微调(Fine-Tuning)的方法对给定数据集进行了文 本分类。通过对验证集的分类效果评估比较，我们得到了适用于当前数据集上最佳的模型超参数组合。随后，我们将模型与其他几种经典文本分类模型(支持向量机、MLP、TextCNN、Bert)的分类效果进行了比较，进一步验证了预训练语言模型和网络构架对文本分类效果的巨大影响。</p>
<p><strong>关键字：文本分类，支持向量机，多层感知机，TextCNN，Bert，XLNet</strong></p>
<h2 id="项目介绍">项目介绍</h2>
<h3 id="任务介绍">任务介绍</h3>
<p>在本项目中，我们需要实现一种机器学习模型，实现对文本的多分类任务。具体来说，给定一组文本集 <span class="math inline">\(\mathcal{D} = \{\boldsymbol{x}_1, \boldsymbol{x}_2, \cdots, \boldsymbol{x}_n \}\)</span> 和类别集 <span class="math inline">\(\mathcal{C} = \{c_1, c_2, \cdots, c_k \}\)</span>，我们需要让机器学习出一种映射 <span class="math inline">\(f: \mathcal{D} \to \mathcal{C}\)</span>，使得对于任意 <span class="math inline">\(\boldsymbol{x} \in \mathcal{C}\)</span>，存在一个 <span class="math inline">\(c \in \mathcal{C}\)</span>，使得 <span class="math inline">\(f(\boldsymbol{x}) = c\)</span>。</p>
<h3 id="数据集介绍">数据集介绍</h3>
<p>本实验的数据集共分为训练集和测试集两个部分。其中训练集包含了8000条各类别的带标签文本，测试集包含了2000条待预测的不含标签文本。训练集中各类别文本的词云如下图所示。通过词云我们可以大致推断出每个类别的主题，例如Class 0的主题可能为<strong>电视剧/电影</strong>，Class 1的主题可能为<strong>手机应用</strong>，Class 2的主题可能为<strong>汽车相关产品</strong>。这为我们后续的分类结果提供了一个人工的验证标准。</p>
<figure>
<img src="dataset_wordcloud.png" srcset="/img/loading.gif" lazyload alt="dataset_wordcloud" /><figcaption aria-hidden="true">dataset_wordcloud</figcaption>
</figure>
<h2 id="基准模型">基准模型</h2>
<p>为了评估目标模型的文本分类效果，我们首先需要一组<strong>基准模型（Baseline Model）</strong>。在本实验中，我们使用了支持向量机、多层感知机、TextCNN以及Bert四种模型作为基准模型。</p>
<h3 id="支持向量机">支持向量机</h3>
<p><strong>支持向量机（Support Vector Machine）</strong>是一个经典的机器学习分类器，其通过计算最优分隔超平面来对向量空间中线性可分的点进行分类。通过使用<strong>核技巧（Kernel Trick）</strong>，其还可以进一步分隔线性不可分的数据集并取得不错的效果。</p>
<p>由于支持向量机仅作用于向量空间，要使得其能够对文本进行分类，我们就需要先对文本进行<strong>嵌入（Embedding）</strong>操作，使其能够在向量空间中表示。一个最朴素的方法便是使用<strong>计数方法</strong>，将词典中每个词在每句话中出现的次数作为句子的表示向量。然而，这样做并不能利用词和词之间的分布特征信息，因此分类效果并不好。一个更为合适的方法是使用所谓的<strong>TF-IDF嵌入法</strong>。</p>
<p>对于一个文本数据集，词项 <span class="math inline">\(t\)</span> 在文档 <span class="math inline">\(d\)</span> 中的TF-IDF值被定义为 <span class="math display">\[
TF-IDF(t, d) = TF(t, d) \times IDF(t)
\]</span> 其中 <span class="math inline">\(TF(t, d)\)</span> 为词项 <span class="math inline">\(t\)</span> 在文档 <span class="math inline">\(d\)</span> 中的<strong>词频（Term Frequency）</strong>，<span class="math inline">\(IDF(t)\)</span> 为词项 <span class="math inline">\(t\)</span> 在整个数据集中的<strong>逆文档频率（Inverse Document Frequency）</strong>。由于测试集中可能出现训练集中不存在的词项，我们对 <span class="math inline">\(IDF\)</span> 引入<strong>拉普拉斯平滑（Laplace Smoothing）</strong>来给这些不存在的词项一个默认的TF-IDF值，此时的 <span class="math inline">\(IDF\)</span> 计算公式可以写为 <span class="math display">\[
IDF(t) = \log \frac{1 + n}{1 + DF(t)} + 1
\]</span> 其中 <span class="math inline">\(n\)</span> 为文档个数，<span class="math inline">\(DF(t)\)</span> 为词项 <span class="math inline">\(t\)</span> 在数据集中出现的文档数。我们只要对句子中所有的词计算其在数据集中的TF-IDF值，即可得到该句子的TF-IDF向量表示。</p>
<p>在本实验中，我们使用了机器学习包<strong>Scikit-Learn</strong>提供的<strong>SVC（SVM Classifier）</strong>类实现了文本分类模型。对于核技巧，我们使用了<strong>高斯核（Gaussian Kernel）</strong>作为核函数，也即 <span class="math display">\[
\boldsymbol{K}(\boldsymbol{x}, \boldsymbol{y}) =  = \exp \left( - \gamma || \boldsymbol{x} - \boldsymbol{y} ||^2 \right)
\]</span> 该核函数被广泛地用于向量空间的特征分离任务中并通过能取得不错的效果。通常情况下，我们需要使用<strong>网格搜索（Grid Search）</strong>的方法来确定核函数中的超参数 <span class="math inline">\(\gamma\)</span>，然而当我们无法确定合理的参数范围时，搜索空间可能会变得十分庞大。由于从直观上看，高斯核中的参数 <span class="math inline">\(\gamma\)</span> 表征了数据集在向量空间中的分散程度，因此一个更为合理的方法便是使用数据集本身的方差来决定该超参数，也即 <span class="math display">\[
\gamma = \frac{1}{n_{feature} \cdot Var(\boldsymbol{X})}
\]</span> 其中 <span class="math inline">\(n_{feature}\)</span> 为数据集的特征维度，<span class="math inline">\(Var(\boldsymbol{X})\)</span> 为数据集的方差。这也是我们的实现中所使用的超参数决定方法。</p>
<h3 id="多层感知机">多层感知机</h3>
<p><strong>多层感知机（Multilayer Perceptron）</strong>又称为<strong>前向全连接神经网络（Feedforward Fully-Connected Neural Network）</strong>，是一种经典的深度学习网络构架。通过多个隐藏层对输入数据中不同粒度特征的提取，其就可以实现对数据的分类。感知机由一个个神经元构成，其单个神经元结构如下图所示。</p>
<figure>
<img src="fcnn_neuron.png" srcset="/img/loading.gif" lazyload alt="fcnn_neuron" /><figcaption aria-hidden="true">fcnn_neuron</figcaption>
</figure>
<p>将多个神经元一层层地连接起来，便构成了多层感知机。对于当前数据集和任务，我们设计的神经网络结构如下图所示。对于输入层，我们同样使用上述SVM模型中提到的TF-IDF嵌入法得到句子的向量表示。经过统计，训练集中不同的词项个数共有 <span class="math inline">\(29999\)</span>个，因此我们将输入层的神经元个数设置为 <span class="math inline">\(29999\)</span>。随后，我们使用两层隐藏层结构来增加网络的非线性拟合能力，其神经元个数分别为 <span class="math inline">\(64\)</span> 和 <span class="math inline">\(32\)</span> 。最后，由于我们有 <span class="math inline">\(10\)</span> 个文本类别，因此输出神经元个数设置为 <span class="math inline">\(10\)</span>。</p>
<figure>
<img src="fcnn_structure.png" srcset="/img/loading.gif" lazyload alt="fcnn_structure" /><figcaption aria-hidden="true">fcnn_structure</figcaption>
</figure>
<p>我们选择了<strong>ReLU（Rectified Linear Unit）</strong>作为当前网络的隐藏层激活函数。对于最后的分类结果，我们使用<strong>Softmax</strong>函数将其放缩为 <span class="math inline">\((0,1)\)</span> 之间的概率值，其定义如下： <span class="math display">\[
\textrm{Softmax}(\boldsymbol{x}_i, \{\boldsymbol{x}_1, \cdots, \boldsymbol{x}_n \}) = \frac{e^{\boldsymbol{x}_i}}{\sum_{j = 1}^n e^{\boldsymbol{x}_j}}
\]</span> 相应的，我们选择了<strong>交叉熵损失（CrossEntropyLoss）</strong>作为当前任务的损失函数，其定义为 <span class="math display">\[
\mathcal{H}(\boldsymbol{y}^{pred}, \boldsymbol{y}^{true}) = - \sum_{i = 1}^n y^{true}_i \log y^{pred}_i
\]</span> 。</p>
<p>神经网络通常使用<strong>反向传播（Backward Propagation）</strong>算法来对网络进行训练，其具体细节在此不再赘述。我们选择了<strong>AdamW</strong>优化器作为当前网络的优化器。AdamW作为Adam优化器的改进版本，相比<strong>随机梯度下降（Stochastic Gradient Descent）</strong>算法能够自适应地调整梯度下降的速率，并能够从一定程度上对抗网络落入<strong>局部最低点（Local Minima）</strong>的情况发生。而相比Adam，AdamW通过一种改进的实现方法解决了其正则化难以收敛的问题，使得网络参数能够更快地收敛到预期的位置。在超参数的设置上，我们通过网格搜索的方式对<strong>学习率（Learning Rate）</strong>进行了搜索，并最终确定学习率 <span class="math inline">\(lr = 0.01\)</span>。对于迭代次数，我们发现当网络迭代10次后 网络的损失不再出现明显的下降，因此我们设置迭代次数 <span class="math inline">\(epoch=10\)</span>。</p>
<h3 id="textcnn">TextCNN</h3>
<p>受到Word2Vec等词嵌入方法和图像处理任务中卷积方法的启发，Yoon Kim等人于2014年提出了用于文本分类的<strong>TextCNN</strong>方法。TextCNN方法本质上是一个<strong>N-Gram</strong>语言模型，这一方法将句子的词向量嵌入矩阵视为一个一维的<strong>特征图</strong>，通过使用多个不同大小的<strong>一维卷积核</strong>在词向量矩阵上做滑动，就可以获得句子中不同粒度上的语言特征。</p>
<figure>
<img src="textcnn_illustrate.png" srcset="/img/loading.gif" lazyload alt="textcnn_illustrate" /><figcaption aria-hidden="true">textcnn_illustrate</figcaption>
</figure>
<p>通常来说，TextCNN由<strong>嵌入层（Embedding Layer）</strong>、<strong>卷积层（Convolutional Layer）</strong>、<strong>池化层（Max-Pooling Layer）</strong>和<strong>全连接层（Fully-Connected Layer）</strong>组成，其中所有的权重参数都会从数据集中学习得到。然而，由于当前任务中数据集规模并不大，网络可能会出现难以收敛或过拟合的问题。受到图像处理任务中<strong>迁移学习（Transfer Learning）</strong>方法的启发，我们使用一个经过预训练的词嵌入词典来直接作为嵌入层的输出，这样网络在初始化时，每个词的语义关系就能够被明确，网络也能够快速找出合适的分类特征。在当前项目中，我们使用了经过无监督预训练的<strong>GloVe</strong>通用词向量词典来对文本进行词嵌入。该词典使用<strong>Common Crawl</strong>数据集进行无监督训练，共包含了190万个词项，嵌入维度为300维，具有足够的通用性和较为合适的特征大小。</p>
<p>对于网络构架，我们参考了Ye Zhang等人的结果，使用了大小分别为<strong>2、3、4</strong>的卷积核，每种卷积核各有<strong>16</strong>个。随后，我们使用和多层感知机相同的<strong>ReLU</strong>作为激活函数，并通过池化核大小为 <span class="math inline">\(l_{sentence} - l_{kernel} + 1\)</span> 的池化层将其每个特征特征压缩至1个神经元内（其中 <span class="math inline">\(l_{sentence}\)</span> 为句子长度，<span class="math inline">\(l_{kernel}\)</span> 为卷积核大小）。最后，通过将这些特征神经元进行拼接并通过一层全连接层，网络即可输出文本的分类概率结果。</p>
<p>为了保证基准模型的公平性，我们将损失函数及优化器设置与上文中的多层感知机保持一致。对于迭代次数，我们发现当网络迭代20次后损失不再出现明显的下降，因此我们设置迭代次数 <span class="math inline">\(epoch=20\)</span>。</p>
<h3 id="bert">Bert</h3>
<p>随着<strong>语言模型（Language Model）</strong>和<strong>Transformer</strong>架构的提出，<strong>预训练（Pretrain）</strong>+<strong>微调（Finetune）</strong>方法逐渐成为了自然语言处理任务中的主流。其中最为经典的就是Google于2018年提出的<strong>Bert（Bidirectional Encoder Representation from Transformers）</strong>模型。</p>
<p>Bert模型是一个<strong>自编码语言模型（Autoencoder Language Model）</strong>，其网络构架如下图所示。Bert的主体由多个Transformer结构组成，网络首先接受一串由<strong>词项</strong>和<strong>特殊标识符（CLS、SEP）</strong>经过嵌入得到的句子向量，并使用Transformer构架计算句子的语言特征，得到一串同等长度的语言特征向量。通过将网络的输出与下游的网络结构相连接，我们就可以使用Bert模型进行各种自然语言处理任务。对于当前文本分类任务，我们只需要使用网络输出中的第一项（即CLS标识符的特征表示），并将其连接至一个全连接层即可得到文本的概率分类结果。</p>
<figure>
<img src="bert_classify.png" srcset="/img/loading.gif" lazyload alt="bert_classify" /><figcaption aria-hidden="true">bert_classify</figcaption>
</figure>
<p>我们使用了Google提供的<strong>bert-base-uncased</strong>预训练模型作为网络构架及初始化参数，这一构架包含<strong>12</strong>个Transformer模块，每个Transformer模块中包含<strong>12</strong>个自注意力头以及一个维度为<strong>768</strong>的全连接层（共110万网络参数），并同样使用了AdamW作为网络的优化器。由于微调训练通常具有较小的网络梯度，我们将学习率设置为了 <span class="math inline">\(lr=5e-5\)</span>。</p>
<h2 id="xlnet文本分类器">XLNet文本分类器</h2>
<p>XLNet是Google于2019年提出的一种对Bert的改进模型。通过融合GPT中使用的<strong>自回归语言模型（Autoregressive Language Model）</strong>和Bert中使用的自编码语言模型并引入更多的构架改进，XLNet在许多下游任务中都获得了十分出色的表现，也是我们在当前实验中最终使用的文本分类模型。</p>
<h3 id="模型构架">模型构架</h3>
<p>XLNet的整体模型构架与Bert模型类似，其都由多组Transformer模块组成。然而，XLNet在进行预训练时的逻辑和Bert有很大的不同，其使用的自回归语言模型相比Bert可以解决其预训练/微调时的数据分布不一致的问题。而通过引入<strong>轮换语言模型（Permutation Language Modeling）</strong>，XLNet就能够在自回归语言模型中保留Bert中的双向语义视野的能力。为了在网络结构中实现这一点，XLNet在实现自注意力层时也使用了不同的方法，其基本构架如下图所示。可以看到，在实际实现过程中，XLNet并没有真正的对文本进行轮换，而是使用了<strong>掩盖（Masking）</strong>的方法来使得网络在预训练时无法得到目标词项的词义信息。</p>
<figure>
<img src="xlnet_architecture.png" srcset="/img/loading.gif" lazyload alt="xlnet_architecture" /><figcaption aria-hidden="true">xlnet_architecture</figcaption>
</figure>
<h3 id="模型实现">模型实现</h3>
<p>由于该模型的实现细节较多且不是本文的重点，我们直接使用了<strong>HuggingFace</strong>提供的<strong>transformers</strong>工具包中所实现的XLNet模型。对于网络构架和参数，我们使用了HuggingFace提供的<strong>xlnet-large-cased</strong>进行初始化。相比<strong>xlnet-base-cased</strong>以及Bert的同级别模型，XLNet Large网络使用了更多的数据进行预训练，使得网络的语言特征捕捉能力得到了进一步的提升，其更大的网络结构（340万网络参数）也保证了网络足够的泛化能力，使得其不会出现过拟合的情况。</p>
<p>与使用Pytorch训练其他模型时一致，我们需要定义一个<strong>数据集对象（Dataset）</strong>来作为训练及测试时的数据源。在数据集初始化中，我们首先使用封装好的<strong>XLNetTokenizer</strong>对象对文本进行词条化及文本嵌入。通过对数据集的检查，我们发现当前数据集中文本的最大长度约为<strong>257</strong>个词项，因此我们将文本<strong>截断/补全长度</strong>设置为了<strong>256</strong>，这也作为了网络的输入尺寸。随后，通过覆写相应的数据集方法，我们就能得到一个可用的文本数据集对象。具体代码片段如下：<strong>（utils.py）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, XLNetTokenizer<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TextDatasetForTrainer</span>(<span class="hljs-params">Dataset</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, source_df, model_type</span>):</span><br>        self.model_type = model_type<br>        <span class="hljs-keyword">if</span> model_type == <span class="hljs-string">&#x27;bert&#x27;</span>:<br>            self.tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)<br>        <span class="hljs-keyword">elif</span> model_type == <span class="hljs-string">&#x27;xlnet&#x27;</span>:<br>            self.tokenizer = XLNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;xlnet-large-cased&#x27;</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;label&#x27;</span> <span class="hljs-keyword">in</span> source_df:<br>            self.mode = <span class="hljs-string">&#x27;paired&#x27;</span><br>            self.labels = [label <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> source_df[<span class="hljs-string">&#x27;label&#x27;</span>]]<br>        <span class="hljs-keyword">else</span>:<br>            self.mode = <span class="hljs-string">&#x27;unpaired&#x27;</span><br>        self.texts = [self.tokenizer(text, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>, max_length=<span class="hljs-number">256</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>                      <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> source_df[<span class="hljs-string">&#x27;text&#x27;</span>]]<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-keyword">return</span> len(self.texts)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self, index</span>):</span><br>        item_dict = self.texts[index]<br>        <span class="hljs-keyword">if</span> self.model_type == <span class="hljs-string">&#x27;xlnet&#x27;</span> <span class="hljs-keyword">or</span> self.model_type == <span class="hljs-string">&#x27;bert&#x27;</span>:<br>            item_dict[<span class="hljs-string">&#x27;attention_mask&#x27;</span>] = item_dict[<span class="hljs-string">&#x27;attention_mask&#x27;</span>].squeeze(<span class="hljs-number">0</span>)<br>            item_dict[<span class="hljs-string">&#x27;input_ids&#x27;</span>] = item_dict[<span class="hljs-string">&#x27;input_ids&#x27;</span>].squeeze(<span class="hljs-number">0</span>)<br>            item_dict[<span class="hljs-string">&#x27;token_type_ids&#x27;</span>] = item_dict[<span class="hljs-string">&#x27;token_type_ids&#x27;</span>].squeeze(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">if</span> self.mode == <span class="hljs-string">&#x27;paired&#x27;</span>:<br>            item_dict[<span class="hljs-string">&#x27;labels&#x27;</span>] = torch.tensor(self.labels[index])<br>        <span class="hljs-keyword">return</span> item_dict<br></code></pre></td></tr></table></figure>
<p>transformers包中提供了一组统一的模型训练调度器，我们可以直接通过创建训练器对象并传入相关参数来对模型进行训练。在<strong>TrainingArguments</strong>对象中，我们可以定义网络训练时的各种属性及网络、优化器的各种超参数。我们只需要将配置好的TrainingArguments对象传递给<strong>Trainer</strong>对象，即可开始对网络进行训练、评估和预测操作。相关代码片段如下：<strong>（xlnet_train.py）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer, TrainingArguments<br><br>train_args = TrainingArguments(<br>    output_dir=<span class="hljs-string">&#x27;output&#x27;</span>,<br>    evaluation_strategy=<span class="hljs-string">&#x27;epoch&#x27;</span>,<br>    save_strategy=<span class="hljs-string">&#x27;no&#x27;</span>,<br>    eval_steps=<span class="hljs-number">50</span>,<br>    per_device_train_batch_size=batch_size,<br>    per_device_eval_batch_size=batch_size,<br>    gradient_accumulation_steps=<span class="hljs-number">1</span>,<br>    num_train_epochs=epoch,<br>    seed=<span class="hljs-number">0</span>,<br>    logging_strategy=<span class="hljs-string">&#x27;step&#x27;</span>,<br>    logging_steps=<span class="hljs-number">64</span>,<br>    <span class="hljs-comment"># Other parameters to set</span><br>)<br>trainer = Trainer(<br>    model=model,<br>    args=train_args,<br>    train_dataset=train_dataset,<br>    eval_dataset=val_dataset,<br>    compute_metrics=compute_metrics<br>)<br>trainer.train()<br></code></pre></td></tr></table></figure>
<h3 id="超参调优">超参调优</h3>
<h4 id="batch-size">Batch Size</h4>
<p>首先我们来确定训练时的Batch Size。在使用随机梯度下降类算法对网络进行优化时，Batch Size往往决定了网络的<strong>收敛速度</strong>及<strong>泛化能力</strong>，过小的Batch Size可能导致网络难以收敛（Loss波动剧烈），而过大的Batch Size可能导致网络落入<strong>Sharp Minima</strong>，使其泛化能力降低（Loss无法降低）。在当前数据集上，不同Batch Size在训练时的Loss变化及在验证集上的正确率表现如下图所示。我们发现，当 <span class="math inline">\(Batch Size &gt; 16\)</span> 时，Training Loss就可以表现的较为稳定；而在相同的迭代次数下，<span class="math inline">\(Batch Size = 64\)</span> 时网络获得了最好的Training Loss表现。</p>
<figure>
<img src="batchsize_loss_acc.png" srcset="/img/loading.gif" lazyload alt="batchsize_loss_acc" /><figcaption aria-hidden="true">batchsize_loss_acc</figcaption>
</figure>
<p>可以发现，由于XLNet模型本身的语言特征捕捉能力及预训练的存在，Batch Size本身对网络的表现影响并不明显。然而在实际应用场景下中，我们还需要考虑训练模型时的资源消耗情况。在使用了<strong>梯度累计（Gradient Accumulation）</strong>技巧时，显存占用随Batch Size的增长情况如下图所示。随着Batch Size的增长，训练XLNet时的显存占用急剧增长，在Batch Size=128时显存占用甚至达到了近50GB，这一资源消耗代价在平日的应用场景下显然是难以接受的。</p>
<figure>
<img src="gpu_mem.png" srcset="/img/loading.gif" lazyload alt="gpu_mem" /><figcaption aria-hidden="true">gpu_mem</figcaption>
</figure>
<p>综合上面两种因素考虑，我们将训练时的Batch Size确定为<strong>64</strong>（或<strong>Batch Size=32 + Gradient Accumulation=2</strong>）。</p>
<h4 id="学习率">学习率</h4>
<p>由于我们使用了AdamW作为网络的优化器，因此每个参数的<strong>学习率（Learning Rate）</strong>会随着梯度的变化而不断发生变化。尽管如此，我们仍然需要决定一个整体学习率以确定各个参数学习率的变化尺度（由于后文所要介绍的学习率调度器的存在，这一学习率实际上为一个初始化学习率）。在当前数据集上，训练时的Loss随学习率的变化情况如下图所示（使用相同的学习率调度器）。可以看到，当学习率过大时，Loss的变化幅度巨大且难以收敛，而当学习率过小时，尽管Loss能够很快达到一个较低的水平，但由于每次优化幅度过小，使得其最终也难以收敛到一个较为合适的水平。</p>
<figure>
<img src="loss_lr.png" srcset="/img/loading.gif" lazyload alt="loss_lr" /><figcaption aria-hidden="true">loss_lr</figcaption>
</figure>
<p>根据实验结果，最终我们将学习率设定为了<strong>5e-5</strong>。</p>
<h4 id="学习率调度器">学习率调度器</h4>
<p>在训练较为庞大的神经网络模型时，我们通常会在训练的不同阶段赋予优化器不同的基准学习率，这就需要一个较为合适的<strong>学习率调度器（Learning Rate Scheduler）</strong>。常用的学习率调度器包含恒定调度器、线性调度器、指数调度器、阶梯形调度器等，其对于学习率的影响如下图所示。可以看到，优化器的学习率在不同学习率调度器的控制下所表现出的变化情况呈现出不同的特征。</p>
<figure>
<img src="scheduler_lr.png" srcset="/img/loading.gif" lazyload alt="scheduler_lr" /><figcaption aria-hidden="true">scheduler_lr</figcaption>
</figure>
<p>在实际的实验过程中，由于AdamW优化器本身的特性，学习率调度器对训练时Loss的变化影响及模型在验证集上的预测正确率并没有显著的影响。最终我们使用<strong>线形（Linear）调度器</strong>作为训练过程中的学习率调度器。</p>
<h4 id="迭代次数">迭代次数</h4>
<p>最后我们来确定训练时的<strong>迭代次数（Iterations）</strong>。通常来说，对于预训练语言模型的微调任务，我们会使用一个较小的迭代次数以防止其过拟合。在当前数据集上，训练时的Loss及模型在验证集上的分类正确率表现如下图所示。可以发现，当 <span class="math inline">\(epoch&lt;4\)</span> 时，网络在验证集上的正确率表现快速提升，而当 <span class="math inline">\(epoch &gt; 4\)</span>时，其正确率表现不再出现明显的提升，甚至开始出现下降。</p>
<figure>
<img src="epoch_loss_acc.png" srcset="/img/loading.gif" lazyload alt="epoch_loss_acc" /><figcaption aria-hidden="true">epoch_loss_acc</figcaption>
</figure>
<p>因此，我们将训练时的迭代次数设定为<strong>4</strong>。</p>
<h2 id="交叉验证">交叉验证</h2>
<p>在验证机器学习模型时，模型的表现有很大一部分来自于数据集本身。若使用常规的方法固定的划分训练集和验证集，则可能会造成模型的表现带有<strong>偏向性（Bias）</strong>。因此，我们通常会使用<strong>交叉验证（Cross Validation）</strong>的方式来验证模型的性能。</p>
<p>交叉验证通常有两种方法。一种被称为<strong>留一法（Leave-one-out Cross Validation）</strong>，这种方法的问题在于每次验证集只有一个数据，若数据集规模很大，则需要循环迭代大量的次数。一个更常用的方法被称为<strong>K-折交叉验证（K-Fold Cross Validation）</strong>，其流程如下图所示（Figure ）。具体来说，我们将所有数据集分成K份，每次取其中一份作为验证集，其他的作为训练数据，依次对模型进行训练和测试，得到 <span class="math inline">\(K\)</span> 个验证集上的评价指标。</p>
<figure>
<img src="cross_val.png" srcset="/img/loading.gif" lazyload alt="cross_val" /><figcaption aria-hidden="true">cross_val</figcaption>
</figure>
<p>随后，我们使用取平均值的方式得到最终的模型评价指标 <span class="math display">\[
Score = \frac{1}{K} \sum_{i = 1}^K Score_i
\]</span></p>
<p>Scikit-Learn提供了一个K-折交叉验证的数据集索引生成器，我们可以直接利用其来完成这一工作。这里我们选择 <span class="math inline">\(K = 5\)</span>。具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> KFold<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>kf = KFold(n_splits=<span class="hljs-number">5</span>, shuffle=<span class="hljs-literal">True</span>)<br>accuracy_list, precision_list, recall_list, f1_list = [], [], [], []<br>fold_cnt = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> train_index, test_index <span class="hljs-keyword">in</span> kf.split(data_df):<br>    fold_cnt += <span class="hljs-number">1</span><br>    train_df = data_df.iloc[train_index]<br>    val_df = data_df.iloc[test_index]<br>    <span class="hljs-comment"># Irrelevant codes</span><br>    accuracy_list.append(eval_metrics[<span class="hljs-string">&#x27;eval_accuracy&#x27;</span>])<br>    precision_list.append(eval_metrics[<span class="hljs-string">&#x27;eval_precision&#x27;</span>])<br>    recall_list.append(eval_metrics[<span class="hljs-string">&#x27;eval_recall&#x27;</span>])<br>    f1_list.append(eval_metrics[<span class="hljs-string">&#x27;eval_f1&#x27;</span>])<br>accuracy_cross = np.array(accuracy_list).mean()<br>precision_cross = np.array(precision_list).mean()<br>recall_cross = np.array(recall_list).mean()<br>f1_cross = np.array(f1_list).mean()<br>print(<span class="hljs-string">&#x27;Average: Validation Accuracy: &#123;0&#125;, Precision: &#123;1&#125;, Recall: &#123;2&#125;, F1: &#123;3&#125;&#x27;</span>.format(accuracy_cross, precision_cross, recall_cross, f1_cross))<br></code></pre></td></tr></table></figure>
<h2 id="效果对比">效果对比</h2>
<p>我们使用<strong>Scikit-Learn</strong>及<strong>Pytorch</strong>工具包实现了本文中提到的各个模型，并使用上述的5折交叉验证方法对模型的分类效果进行了评估<strong>（具体代码请参考随附的README.md说明文档）</strong>。各个模型在训练集上的5折交叉验证结果如下表所示。可以看出，经过超参调优的XLNet模型在各个指标上均取得了所有模型中最好的分类表现，Bert和多层感知机模型的表现紧随其后，而传统的SVM分类器和TextCNN模型的表现则较为落后。</p>
<figure>
<img src="evaluation_score.png" srcset="/img/loading.gif" lazyload alt="evaluation_score" /><figcaption aria-hidden="true">evaluation_score</figcaption>
</figure>
<p>进一步的，各个模型在各折验证结果中的正确率变化如下图所示。可以看出，基于传统机器学习和通用深度学习方法的分类模型在验证集上的正确率波动较大，而基于预训练+微调的语言模型分类方法则有着较好的稳定性。</p>
<figure>
<img src="models_acc_fold.png" srcset="/img/loading.gif" lazyload alt="models_acc_fold" /><figcaption aria-hidden="true">models_acc_fold</figcaption>
</figure>
<p>最后，我们使用所有模型中效果最好的经过调优的XLNet模型对目标测试集进行了文本类别预测，其结果可见随附的<strong>test_output.txt</strong>文件。</p>
<h2 id="总结">总结</h2>
<p>在本实验中，我们实现了基于XLNet语言模型的文本分类器，并对其各个超参数进行了调优使其在验证集上得到较好的效果。随后，我们将其与其他几种经典文本分类模型的分类效果进行了对比，探索了几种不同的文本分类模型的特征及其在目标数据集上的表现。最后，我们使用训练完成的文本分类模型对目标测试集进行了标签预测，达到了预期的实验要求。</p>
<h2 id="references">References</h2>
<ol type="1">
<li>Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for optimal margin classifiers. In Proceedings of the Fifth Annual Workshop on Com- putational Learning Theory, COLT ’92, page 144–152, New York, NY, USA, 1992. Association for Computing Machinery.</li>
<li>Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.</li>
<li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre- training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Com- putational Linguistics.</li>
<li>Ben Dickson. What are artificial neural networks (ann)? https://bdtechtalks. com/2019/08/05/what-is-artificial-neural-network-ann/, 2019.</li>
<li>Yoon Kim. Convolutional neural networks for sentence classification. In Proceed- ings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar, October 2014. Association for Computa- tional Linguistics.</li>
<li>Scikit learn Developers. Cross-validation: evaluating estimator performance. https://scikit-learn.org/stable/modules/cross_validation.html, 2021.</li>
<li>Scikit learn Developers. Feature extraction. https://scikit-learn.org/stable/ modules/feature_extraction.html, 2021.</li>
<li>Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101, 2017.</li>
<li>Katherine (Yi) Li. How to choose a learning rate scheduler for neural networks. https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler, 2021.</li>
<li>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, 2014.</li>
<li>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics.</li>
<li>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.</li>
<li>Ye Zhang and Byron C. Wallace. A sensitivity analysis of (and practitioners’guide to) convolutional neural networks for sentence classification. In IJCNLP, 2017.</li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">当代人工智能</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Computer-Science/">Computer-Science</a>
                    
                      <a class="hover-with-bg" href="/tags/Artificial-Intelligence/">Artificial-Intelligence</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/12/24/machine-learning/dase-alg-exp-summary/">
                        <span class="hidden-mobile">数据科学与工程算法基础 文本摘要实验</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
