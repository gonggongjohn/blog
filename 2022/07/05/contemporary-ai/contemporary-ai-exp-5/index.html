

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="GONGGONGJOHN">
  <meta name="keywords" content="">
  
    <meta name="description" content="摘要 多模态学习是近几年来机器学习中的一个较为新兴的领域，不同模态的组合和应用场景延伸出了一系列多模态学习任务。其中，多模态情感分析作为一个经典的多模态任务，有着极为广泛的应用，也是现如今建立多模态模型的一项常用的基准评价任务。在本文中，我们从零开始设计并实现了一个基于Transformer构架的文本-图像双模态情感分析模型，并使用预训练和对比学习方法来提升其分类效果。通过与一系列经典及较新的单模">
<meta property="og:type" content="article">
<meta property="og:title" content="当代人工智能 课程项目五 多模态情感分析实验报告">
<meta property="og:url" content="http://gonggongjohn.me/2022/07/05/contemporary-ai/contemporary-ai-exp-5/index.html">
<meta property="og:site_name" content="GONGGONGJOHN&#39;s Blog">
<meta property="og:description" content="摘要 多模态学习是近几年来机器学习中的一个较为新兴的领域，不同模态的组合和应用场景延伸出了一系列多模态学习任务。其中，多模态情感分析作为一个经典的多模态任务，有着极为广泛的应用，也是现如今建立多模态模型的一项常用的基准评价任务。在本文中，我们从零开始设计并实现了一个基于Transformer构架的文本-图像双模态情感分析模型，并使用预训练和对比学习方法来提升其分类效果。通过与一系列经典及较新的单模">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://gonggongjohn.me/img/contai_project5.png">
<meta property="article:published_time" content="2022-07-05T02:00:00.000Z">
<meta property="article:modified_time" content="2022-12-18T10:23:10.284Z">
<meta property="article:author" content="GONGGONGJOHN">
<meta property="article:tag" content="Computer-Science">
<meta property="article:tag" content="Artificial-Intelligence">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://gonggongjohn.me/img/contai_project5.png">
  
  
  <title>当代人工智能 课程项目五 多模态情感分析实验报告 - GONGGONGJOHN&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"gonggongjohn.me","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="GONGGONGJOHN's Blog" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>GONGGONGJOHN&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="当代人工智能 课程项目五 多模态情感分析实验报告">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-07-05 10:00" pubdate>
        2022年7月5日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      16k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      136 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">当代人工智能 课程项目五 多模态情感分析实验报告</h1>
            
            <div class="markdown-body">
              <h2 id="摘要">摘要</h2>
<p>多模态学习是近几年来机器学习中的一个较为新兴的领域，不同模态的组合和应用场景延伸出了一系列多模态学习任务。其中，多模态情感分析作为一个经典的多模态任务，有着极为广泛的应用，也是现如今建立多模态模型的一项常用的基准评价任务。在本文中，我们从零开始设计并实现了一个基于Transformer构架的文本-图像双模态情感分析模型，并使用预训练和对比学习方法来提升其分类效果。通过与一系列经典及较新的单模态和多模态基准模型进行比较，我们得以较为全面的衡量所设计模型的精度和特点。最后，我们通过消融实验，探究了模型在单模态输入下的分类表现，以及预训练为模型来带来的精度提升幅度。</p>
<p>本项目中涉及的所有核心代码均由本人亲自实现，代码中所有借鉴其他模型的地方，报告中已全部给出相关原文或地址。</p>
<p><strong>关键字：多模态情绪识别，Transformer, XLMRoBERTa，Swin Transformer，预训练模型，对比学习</strong></p>
<h2 id="项目介绍">项目介绍</h2>
<h3 id="任务介绍">任务介绍</h3>
<p>传统机器学习领域往往是针对单模态数据来进行处理和分析的，而真实世界中，一个事物的特征往往由多个模态共同组成，在许多场景下，我们需要共同考虑这些特征才能得到一个较好的表征。多模态学习正是在这样的背景下所提出的。传统的多模态学习模型往往由多个单模态模型及一系列融合技巧组成，而随着近年来预训练方法的提出，多模态预训练模型也正逐渐成为多模态理解模型的主流，尤其是在<strong>视觉-语言领域</strong>取得了显著的效果。</p>
<p>在多模态任务中，<strong>多模态情感分析</strong>是一个经典且十分重要的任务。根据模态数据的不同组合，多模态情感分析能够广泛地用于不同的场景任务下，例如产品口碑分析、大众情绪倾向分析及心理健康预警等。由于图像和文本是真实世界中主流且较易处理的数据模态，因此这里我们近考虑这种双模态情况。<strong>图像-文本双模态情感分析</strong>任务的具体表述如下：对于一组<strong>匹配的图像文本对</strong> <span class="math inline">\((I, T) \in \mathcal{D}\)</span>，要求给出一个<strong>情感倾向分类结果</strong> <span class="math inline">\(c \in \mathcal{C}\)</span>，使得该情感是对应图像文本对所表现出的情感倾向。</p>
<h3 id="数据集介绍">数据集介绍</h3>
<p>本项目所使用的数据集为一个给定的数据集，该数据集中共包含了<strong>5129</strong>组匹配的图像-文本对。其中，训练集共有<strong>4000</strong>个样本，每个样本带有一个与之对应的情感分类标签，其具体分布如下表所示；测试集共有<strong>511</strong>个样本，测试集的情感标签需要我们使用模型推理得到。</p>
<table>
<thead>
<tr class="header">
<th>标签</th>
<th>样本数（图像-文本对）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>积极（Positive）</td>
<td>2388</td>
</tr>
<tr class="even">
<td>中性（Neutral）</td>
<td>419</td>
</tr>
<tr class="odd">
<td>消极（Negative）</td>
<td>1193</td>
</tr>
<tr class="even">
<td><strong>总计</strong></td>
<td><strong>4000</strong></td>
</tr>
</tbody>
</table>
<p>在数据集中，一个带标签样本的结构如下图所示。由于数据集是从互联网的非结构化信息中抽取的，数据集中的同质样本规格并不统一，且文字样本中还存在着一些无关信息。可以看到，数据集中图像和文本对于情感倾向的判断均或多或少确实一些信息，需要结合两种模态的特征才能得到一个较为准确的情感倾向分类。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/dataset_visualize.png" srcset="/img/loading.gif" lazyload class="" title="dataset_visualize">
<h2 id="基准模型">基准模型</h2>
<h3 id="单模态模型">单模态模型</h3>
<p>单模态模型仅考虑单一的输入形式，在当前任务中，也即分别考虑文本和图像数据。在本文中，我们共实现了4中单模态基准模型，分别对应于一个<strong>经典Baseline</strong>和一个<strong>强Baseline</strong>。其中，<strong>Bert</strong>和<strong>XLMRoberta</strong>模型仅使用文本数据作为输入，<strong>ResNet</strong>和<strong>Swin Transformer</strong>仅使用图像数据作为输入。</p>
<h4 id="bert">Bert</h4>
<p>使用文本数据进行情感分析，也即根据文本的语义进行多分类任务。近年来，随着<strong>语言模型（Language Model）</strong>和<strong>Transformer构架</strong>的提出，使用<strong>预训练（Pretrain）</strong>方法来处理文本数据已经成为了现代模型的一大主流，其中最为经典的即为Google于2018年提出的<strong>Bert（Bidirectional Encoder Representation from Transformers）</strong>模型。</p>
<p>Bert模型是一个<strong>自编码语言模型（Autoencoder Language Model）</strong>，其网络构架如下图所示。Bert的主体由多个Transformer结构组成，网络首先接受一串由<strong>词项</strong>和<strong>特殊标识符（CLS、SEP）</strong>经过嵌入得到的句子向量，并使用Transformer构架计算句子的语言特征，得到一串同等长度的语言特征向量。通过将网络的输出与下游的网络结构相连接，我们就可以使用Bert模型进行各种自然语言处理任务。对于当前文本分类任务，我们只需要使用网络输出中的第一项（即CLS标识符的特征表示），并将其连接至一个全连接层即可得到文本的概率分类结果。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/bert_structure.png" srcset="/img/loading.gif" lazyload class="" title="bert_structure">
<p>Huggingface的transformers工具提供了一套便捷的类Transformer模型实现工具，我们使用其实现了Bert情感分类模型。对于预训练权重，我们使用<strong>bert-base-cased</strong>进行了初始化，并通过连接一个 <span class="math inline">\(768 \to 3\)</span> 的<strong>线性层（Linear）</strong>进行了微调。</p>
<h4 id="xlmroberta">XLMRoBERTa</h4>
<p>通过进一步分析数据集我们可以发现，该数据集中存在多种语言的文本数据，如下图所示。而传统的类Bert模型通常仅针对单语料进行训练，其词条化规则也通常不能适应不同语言的语言特性。为了解决这一问题，一个传统的方法是人工找出数据集中出现的所有语言，并使用在对应语言的语料上训练的模型进行推理。然而，这样做的编码和推理复杂度降大大增加，且不具有自适应性。幸运的是，随着多语言推理需求的增加，同时支持多种语言的类Bert模型相继被提出，这使得我们可以通过一个统一的模型来对数据集中的所有样本进行推理。这里我们使用Facebook团队于2019年提出的<strong>XLMRoBERTa</strong>模型作为基准模型。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/dataset_multilingual.png" srcset="/img/loading.gif" lazyload class="" title="dataset_multilingual">
<p>XLMRoBERTa是一个多语言预训练模型，通过名字就可以看出，其是一个RoBERTa和XLM的结合体。RoBERTa通过对预训练任务的改进和参数的调优，大幅提升了Bert的语义特征提取能力；XLM通过对文本嵌入方法的改进，使得多语言文本可以通过一种统一的方法输入模型。在此基础之上，XLMRoBERTa使用了多语言的训练语料对模型进行了训练，其语料的具体语言分布如下图所示。可以看到，整个训练语料超过了2.5TB，这也使得模型在面对不同语言输入时均能获得良好的嵌入表现。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/xlmroberta_corpus.png" srcset="/img/loading.gif" lazyload class="" title="xlmroberta_corpus">
<p>本次实验中，我们使用官方提供的<strong>xlm-roberta-base</strong>作为基准模型的预训练模型。与Bert类似，我们将模型输出的[CLS]特征连接至一个 <span class="math inline">\(768 \to 3\)</span> 的线性层进行了微调。</p>
<h4 id="resnet">ResNet</h4>
<p>接下来我们来考虑基于图像输入的基准模型。基于图像的情感分析也即根据图像的特征进行多分类任务，决定其分类表现的核心在于是否有一个较好的视觉特征提取器。ResNet是现代计算机视觉模型中一个里程碑式的网络结构，由<strong>Kaiming He</strong>等人于2016年提出。ResNet通过提出<strong>残差连接（Residue Connection）</strong>这一思想，有效地解决了网络层数加深导致的<strong>输入特征消失</strong>以及<strong>学习偏差</strong>的问题。</p>
<p>ResNet中最重要的部分即为<strong>残差块（Residue Block）</strong>，其基本结构如下图所示。可以看到，相比普通的卷积块，残差块通过加入一个直接连接输入和输出的快速通道来将输入的特征直接叠加到输出特征之上，从而使得输入特征得以保留。从函数的角度上来看，若输入向量为 <span class="math inline">\(\boldsymbol{x}\)</span>，卷积层变换函数为 <span class="math inline">\(g(x)\)</span>，则一个残差块的输出结果为 <span class="math display">\[
f(\boldsymbol{x}) = \boldsymbol{x} + g(\boldsymbol{x})
\]</span> 。可以看到，即使卷积层没有学到任何有用的特征（卷积层退化为常数映射 <span class="math inline">\(g(\boldsymbol{x}) = C\)</span>），输出结果中仍然包含输入的特征，从而有效地防止学习偏差的问题。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/resblock_architecture.png" srcset="/img/loading.gif" lazyload class="" title="resblock_architecture">
<p>在本次实验中，我们使用<strong>ResNet-50</strong>作为基准模型的网络结构。对于预训练权重，我们使用在<strong>ImageNet-1K</strong>上训练得到的参数对网络进行了初始化，并通过连接一个 <span class="math inline">\(2048 \to 3\)</span> 得到最终的类别概率。</p>
<h4 id="swin-transformer">Swin Transformer</h4>
<p>随着Transformer在自然语言处理领域的发展以及对其模型构架的理解深入，其思想也逐渐被迁移到了计算机视觉领域上，并取得了十分优秀的表现，这也使得视觉预训练模型成为了CV领域中的一个新典范。作为一个强Baseline，我们使用Microsoft于2021年提出的<strong>Swin Transformer</strong>作为基准模型，该模型在各种下游任务中均取得了十分优异的表现。</p>
<p>Swin Transformer是一个纯Transformer构架模型，其所使用的核心思想和模型基本构架如下图所示。具体来说，Swin Transformer使用了一个层级式的特征抽取结构，利用滑动窗口逐块地提取每个区域的视觉特征，通过逐步扩大感受野来得到整个图像的视觉特征，这也想法与多层的卷积神经网络十分相似。而又由于Transformer天然的自注意力机制，使得其可以学习到每一个局部感受野中需要重点关注的区域，也使得其能够获得较好的特征提取表现。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/swin_transformer_illustrate.png" srcset="/img/loading.gif" lazyload class="" title="swin_transformer_illustrate">
<p>在本实验中，我们使用<strong>Swin-Base</strong>模型作为基准模型，其输出的分类特征为一个1024维的特征向量，通过一个 <span class="math inline">\(1024 \to 3\)</span> 的线性层，我们可以将其转化为类别概率。对于预训练权重，我们使用其官方提供的在ImageNet-1K上训练的<strong>microsoft/swin-base-patch4-window7-224</strong>模型对网络参数进行了初始化。随后，我们加上线性层，并进行了微调操作。</p>
<h3 id="双模态模型">双模态模型</h3>
<p>接下来我们同时考虑图像和文本的双模态输入，并构建一些符合直觉的基准模型。在本文中，我们实现了两种较为简单的双模态模型：<strong>特征拼接</strong>和<strong>加性模型</strong>，其都能够作为一个较为基础的Baseline。</p>
<h4 id="特征拼接">特征拼接</h4>
<p>要让模型能够同时考虑到多模态的输入，一个关键的问题在于如何使得多模态的特征能够共同影响分类的结果。一个最直接的方法便是将多模态特征直接拼接成一个更大的特征并送入分类器，事实上，早期的多模态模型也大都使用了这种方法，如Nan Xu等人提出的MultiSentiNet等。在本实验中，我们使用如下结构来作为特征拼接的双模态基准模型。文本和图像首先分别通过各自地特征提取器得到对应的嵌入特征，随后通过序列扩展的方式将其拼接成一个更长的特征串。具体来说，若输入文本得到的特征串为 <span class="math inline">\(k\)</span>，输入图像得到的特征串为 <span class="math inline">\(p\)</span>，则拼接后的特征串长度即为 <span class="math inline">\(k+p\)</span>。随后，我们将其直接输入一个 <span class="math inline">\(k+p \to 3\)</span> 的线性层，即可得到类别概率的输出。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/concatenate_structure.png" srcset="/img/loading.gif" lazyload class="" title="concatenate_structure">
<p>为了公平起见，对于文字和图像的特征提取器，我们统一使用<strong>XLMRoBERTa</strong>和<strong>Swin Transformer</strong>作为其骨干网络，并首先使用微调的方式使其能够较好地捕捉数据集上的特征。在进行多模态训练时，我们将两个骨干网络冻结，仅训练全连接层的参数。</p>
<h4 id="加性模型">加性模型</h4>
<p>对于向量空间模型而言，线性变换也是特征结合的一种常用方法。受到注意力机制中加性模型的启发，我们也可以通过对不同模态的特征进行加权求和的方式得到最终的特征串。事实上，不少序列预测模型也使用了加性模型作为其交叉注意力层的打分函数，例如ChuHan Wu等人提出的Fastformer模型。在本实验中，我们使用如下结构作为加性模型的双模态基准模型。所图所示，文本和图像输入首先仍然通过各自的特征提取器得到嵌入标识，并分别送入一个 <span class="math inline">\(k \to q\)</span> 和 <span class="math inline">\(p \to q\)</span> 的线性层使得文本和图像的特征长度能够对齐。随后，对于特征对的每一个位置，我们使用加权求和的方式得到一个模态融合的特征向量。最后，我们将这一特征向量输入一个 <span class="math inline">\(q \to 3\)</span> 的线性层，即可得到最终的类别概率输出。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/additive_structure.png" srcset="/img/loading.gif" lazyload class="" title="additive_structure">
<p>在这一模型中，我们仍然使用和拼接模型中相同的模型作为文本和图像的特征提取骨架。此外，为了增加模型的适应性，我们将加权后的向量输入一个<strong>tanh</strong>激活函数，并通过一个额外的线性变换来得到输出特征。具体来说，加权过程的表示如下：</p>
<p><span class="math display">\[
\boldsymbol{m}_j = \boldsymbol{W} \tanh (\boldsymbol{\alpha}_{j1} t_j&#39; + \boldsymbol{\alpha}_{j2} i_j&#39;), j \in \{1, 2, \cdots, q\}
\]</span></p>
<p>同样地，在训练过程中，我们将骨干网络冻结，仅训练加权网络及线性层的参数。</p>
<h2 id="多模态情感分类模型ptamsc">多模态情感分类模型——PTAMSC</h2>
<p>下面我们将引入一种改进的多模态情感分类模型，我们将其称为<strong>PTAMSC（Pure Transformer-flavored Augmented Multi-modal Sentiment Classifier）</strong>。我们将从模型构架、预训练过程与下游训练过程分别阐述该模型的相关细节。</p>
<h3 id="模型构架">模型构架</h3>
<p>从上面的基准模型中我们可以看出，单模态模型并不能够利用完整的输入信息，使得其情感分类效果存在上限；而使用简易的拼接或加性模型的双模态方法并不能将不同模态地表征数据合理地对其并结合在一起，使得分类效果反而出现了下降。受<strong>Transformer Encoder</strong>的启发，<strong>Cross Attention</strong>机制天然适用于对不同的表征进行融合和关注，这也自然地使得其能够较好地应用到多模态特征融合中。参考Zhen Li等人提出的<strong>CLMLF</strong>模型构架，本文的多模态情感分类模型基本构架如下图所示。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/ptamsc_structure.png" srcset="/img/loading.gif" lazyload class="" title="ptamsc_structure">
<p>首先，文本序列 <span class="math inline">\(\{s_1, s_2, \cdots, s_k \}\)</span> 和原始图像 <span class="math inline">\(I \in \mathbb{R}^{c \times w \times h}\)</span> 分别通过各自的特征提取骨架网络得到对应的特征表示 <span class="math inline">\(\{t1, t_2, \cdots, t_k \}\)</span> 和 <span class="math inline">\(\{i_1, i_2, \cdots, i_p \}\)</span>。随后，我们将其送入一个 <span class="math inline">\(L\)</span> 层的<strong>Transformer编码器</strong>，得到模态融合特征 <span class="math inline">\(\{m_1, m_2, \cdots, m_q \}\)</span>（其中 <span class="math inline">\(q = k + p\)</span>）。由于Transformer编码器得到的特征是分散在各个Token上的，因此我们再将其送入一个新的自注意力层得到整个序列的聚合特征表示 <span class="math inline">\(\{m_1&#39;, m_2&#39;, \cdots, m_q&#39; \}\)</span>。最后，我们将该特征表示连接到一个 <span class="math inline">\(q \to 3\)</span> 的线性层，即可得到最终的类别概率输出。</p>
<p>对于本实验，我们统一使用<strong>xlm-roberta-base</strong>和<strong>swin-base</strong>作为骨干网络的模型构架，因此特征长度 <span class="math inline">\(k = 768, p = 1024\)</span>。为了能够使其统一输入模态融合编码器，我们在实际实现时在图像特征提取器上方额外增加了两层Transformer编码器，这一结构结合下面的对比学习任务从一定程度上也可以实现文字表征和图像表征的<strong>特征对齐</strong>。我们使用<strong>GELU</strong>作为层与层之间非线性变换的激活函数，并使用<strong>点积模型</strong>作为自注意力层的打分计算模型。</p>
<h3 id="预训练任务多模态情感预分类">预训练任务——多模态情感预分类</h3>
<p>由于上面设计的网络是一个全Transformer的结构，借鉴Bert等预训练模型的思想，我们同样可以考虑对该模型进行<strong>预训练</strong>以提升其特征提取能力。通常来说，对模型的预训练可以通过<strong>监督（Supervised）</strong>或<strong>自监督（Self-supervised）</strong>任务来完成，其中前者多用于视觉模型，后者多用于语义模型。然而，一般的图像-文本自监督预训练任务如<strong>图像文本匹配（Image-Text Matching）</strong>等往往考虑的是图文之间内在特征的匹配程度，这与我们共同考虑获得一个外在情感打分的任务并不一致。由于情感分类本质上是一个监督学习任务，因此我们可以直接将这一任务也作为预训练时的任务，通过引入额外的数据集来让模型获得更好的初始特征提取能力，同时也彻底解决了预训练和下游任务不匹配的问题。Yan Ling等人的研究也表明了这一预训练任务本身就具有很强的有效性。</p>
<p>我们使用<strong>MVSA-Multi</strong>数据集对模型进行了预训练。该数据集由Niu Teng等人于2016年提出，是一个经典的双模态情感分析数据集，其具体样本分布及数量如下表所示。由于该数据集中的样本同样为图像+文本，且目标类别数同样为3，因此十分适合作为当前场景下的预训练数据集。</p>
<table>
<thead>
<tr class="header">
<th>标签</th>
<th>样本数（图像-文本对）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>积极（Positive）</td>
<td>11903</td>
</tr>
<tr class="even">
<td>中性（Neutral）</td>
<td>4107</td>
</tr>
<tr class="odd">
<td>消极（Negative）</td>
<td>1500</td>
</tr>
</tbody>
</table>
<p>我们以 <span class="math inline">\(9:1\)</span> 将数据集划分为了训练集和验证集。在预训练阶段，我们同时输入图像、文本和标签对模型进行监督训练，并使用<strong>交叉熵（CrossEntropy）</strong>作为模型的损失函数。对于超参数设置，我们将<strong>Batch Size</strong>设为<strong>32</strong>，特征提取骨架网络学习率设置为<strong>2e-6</strong>，其他部分学习率设置为<strong>5e-5</strong>，使用<strong>AdamW+Linear Scheduler</strong>分别作为模型的参数优化器和学习率规划器，共训练<strong>40</strong>代。随后，我们使用一张<strong>Nvidia A40</strong>显卡作为硬件环境对模型进行了预训练，训练中的Loss和在验证集上的分类准确率如下图所示。可以看到，预训练后的模型在验证集上的分类精度F1值能达到约<strong>0.64</strong>，且Loss随着预训练的过程推进逐步下降，表明该预训练任务及数据集是合适的，且模型在预训练过程中能够正确收敛。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/ptamsc_pretrain_summary.png" srcset="/img/loading.gif" lazyload class="" title="ptamsc_pretrain_summary">
<h3 id="下游推理">下游推理</h3>
<h4 id="对比学习">对比学习</h4>
<p>接下来我们在目标数据集上对模型进行训练。由于深度神经网络本质上是一个多层非线性模型，若直接使用原始数据对其进行训练，则其输出特征的距离语义无法明确。为了让模型能够更好地区分不同情感倾向的样本特征，我们可以采用<strong>对比学习（Contrastive Learning）</strong>的方法来专门对模型的正负样本特征进行强化。对比学习是一种自监督的学习方法，其通过<strong>代理任务（Proxy Task）</strong>将样本划分为正负样本，并尝试找到一种空间表征，使得正样本或负样本间的距离尽可能小，同时又使得正样本和负样本之间的距离足够的大。对于当前任务来说，我们希望拥有相同情感标签的输出表征足够的近，而拥有相反情感标签的输出表征足够的远。因此，对于一个拥有<strong>“积极”</strong>情感标签的样本，我们将所有同批次中标签为<strong>“积极”</strong>的样本均视为正样本，而将同批次中标签为<strong>“中性”</strong>或<strong>“消极”</strong>的样本视为负样本；相反的，对于标签为“消极”的样本，我们则将同批次中标签同为“消极”的样本视为正样本，反之则视为负样本。</p>
<p>我们采用CLMLF中基于标签的对比学习算法来从正负样本中获得对比学习损失。具体来说，对于一组数据中的两个同标签样本 <span class="math inline">\(x_i, x_j \in \mathcal{D}\)</span>，设其通过上述网络结构后的特征向量分别为 <span class="math inline">\(y_i, y_j\)</span>，则其损失函数被定义为</p>
<p><span class="math display">\[
l_{ij} = -\log \frac{\exp \left\{ y_i^T y_j / \tau \right\}}{\sum_{y_k \in \mathcal{D} \land k \neq i} \exp \left\{ y_i^T y_k / \tau \right\} }
\]</span> 其中 <span class="math inline">\(\tau\)</span> 为<strong>温度系数</strong>。这一损失函数本质上即为对比学习中最常用的<strong>InfoNCE Loss</strong>。从公式中我们可以看到，只有当两个正样本间的点积足够小，而所有正负样本对间的点积结果足够大，该损失的结果才会达到最小。</p>
<p>对数据集中所有满足以上条件的样本对计算上述损失并进行加权平均，就得到了整个网络的<strong>对比学习损失</strong>，也即 <span class="math display">\[
\mathcal{L}_{Contrastive} = \frac{1}{|Pair(\mathcal{D})|} \sum_{(i, j) \in Pair(\mathcal{D})} l_{ij}
\]</span></p>
<h4 id="损失函数">损失函数</h4>
<p>由于当前任务是一个监督多分类问题，因此我们使用交叉熵作为损失函数的主干部分。为了提升模型的分类表现，我们将上面提到的对比学习损失通过加权和的方式融入最终的损失函数中。 于是，整个模型的损失函数即为</p>
<p><span class="math display">\[
\mathcal{L} = \lambda_1 \mathcal{L}_{Cross-Entropy} + \lambda_2 \mathcal{L}_{Contrastive}
\]</span> 其中 <span class="math inline">\(\mathcal{L}_{Cross-Entropy}\)</span> 为交叉熵损失，<span class="math inline">\(\mathcal{L}_{Contrastive}\)</span> 为上述的对比学习损失，<span class="math inline">\(\lambda_1\)</span> 和 <span class="math inline">\(\lambda_2\)</span> 分别为两个子损失函数的权重。通过网格搜索，我们将其确定为 <span class="math inline">\(\boldsymbol{\lambda_1 = 0.55, \lambda_2 = 0.45}\)</span>。</p>
<h2 id="实验">实验</h2>
<h3 id="数据集预处理">数据集预处理</h3>
<p>为了使得目标数据集能够适配模型的输入结构并取得较优的效果，我们需要首先对数据集的文字和图像部分分别进行预处理。</p>
<p>通过读入数据集我们可以发现，原始数据集中文本数据所使用的存储编码并不一致，部分文件使用常规的<strong>UTF-8</strong>及<strong>ANSI（cp1252）</strong>格式无法正确读取，如下图所示。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/dataset_encode_error_detail.png" srcset="/img/loading.gif" lazyload class="" title="dataset_encode_error_detail">
<p>我们通过VSCode使用UTF-8编码打开读取错误的文件，发现其存在着大量特殊字符。经过一系列尝试，我们发现这些文件在使用<strong>GBK</strong>编码时均能够正确显示。正确解码后，我们发现其均为日语。为了方便模型统一进行推理，我们将这些文件统一翻译为了英文，并使用UTF-8编码进行保存。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/dataset_errortext_utf8_decode.png" srcset="/img/loading.gif" lazyload class="" title="dataset_errortext_utf8_decode">
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/dataset_errortext_gbk_decode.png" srcset="/img/loading.gif" lazyload class="" title="dataset_errortext_gbk_decode">
<p>接下来我们对数据集中的图像进行预处理。由于数据集是从非结构化数据中获得的，因此每张图像的大小并不一致。为了适配目标神经网络的输入格式，我们需要将其变换至统一的尺寸。由于ImageNet中图像的尺寸为 <span class="math inline">\(\boldsymbol{224 \times 224}\)</span>，本项目中所使用的视觉预训练模型也均为该数据集上训练的模型，因此这里我们也将目标图片调整至这一大小。具体的，我们先将图像等比缩放至最近邻2的倍数大小，随后通过中心裁剪的方式将其变换为目标尺寸。随后，为了让模型更快收敛，我们参考ResNet实现中的建议，将每张图像标准化为一个特定统计特征的张量。具体实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 图像变换</span><br>img_transformer = transforms.Compose(<br>    [<br>        transforms.Resize(img_aligned_scale(target_size)),<br>        transforms.CenterCrop(target_size),<br>        transforms.ToTensor(),<br>        transforms.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])<br>    ]<br>)<br><span class="hljs-comment"># 将图像等比放大至2的倍数</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">img_aligned_scale</span>(<span class="hljs-params">target_size</span>):</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">20</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-number">2</span> ** i &gt;= target_size:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> ** i<br>    <span class="hljs-keyword">return</span> target_size<br></code></pre></td></tr></table></figure>
<p>经过上述预处理后，我们使用Pytorch中提供的<strong>Dataset</strong>和<strong>Dataloader</strong>接口，即可将其组合为模型训练所要求的数据格式。</p>
<h3 id="超参数调优">超参数调优</h3>
<p>特别的，我们使用<strong>网格搜索（Grid Search）</strong>的方法对两个<strong>特征提取骨干网络（Backbone）的微调学习率（lr1）</strong>、<strong>模态融合模型学习率（lr2）</strong>以及<strong>学习率预热步数（WarmUp）</strong>三个超参数的最优值进行了搜索，使用<strong>F1</strong>值作为结果评价指标，结果如下图所示。可以看到，随着微调学习率的增大，网络的训练结果先增大后剧烈降低，并在<strong>lr1=1e-5</strong>时分类表现达到最优；当恒定微调学习率时，网络的训练结果随着融合学习率的增大缓慢上升，并在学习率超过<strong>lr2=5e-3</strong>时出现了较大的波动；学习率预热步数对训练结果的影响较小，随着预热步数的增大，训练结果同样呈现先增大后见小的趋势，当<strong>warmup=10</strong>时，网络取得了最优的分类表现。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/xlm_hypertune.png" srcset="/img/loading.gif" lazyload class="" title="xlm_hypertune">
<p>根据上面的搜索结果，我们将三个超参数确定为<strong>lr1=1e-5, lr2=5e-3, warmup=10</strong>。对于其他超参数，我们采用了<strong>AdamW</strong>作为本文所有模型的优化器，其三个核心超参数被分别设为<strong>beta1=0.9, beta2=0.999, epsilon=1e-8</strong>。为了权衡模型的训练性能需求，我们将<strong>批大小（Batch Size）</strong>统一设置为<strong>32</strong>，训练次数Epoch被设为<strong>10</strong>。</p>
<h3 id="训练过程">训练过程</h3>
<p>我们首先使用上面提到的任务和配置对模型进行了预训练。以该结果作为模型的初始参数，我们继续使用目标数据集进行微调训练任务。对于数据集中的带标签部分，我们按照 <span class="math inline">\(8:2\)</span> 将其划分为了训练集和测试集，并使用上面的探索结果作为训练时的超参数设置。训练时的Loss及模型在验证集上的分类表现随迭代次数的变化情况如下图所示。可以看到，训练Loss在<strong>150</strong>步及<strong>750</strong>步附近均出现了明显的下降，其他区间均在一个特定范围中波动，总体上呈缓慢下降的趋势，表明模型优化成功。随着训练过程的推进，模型在验证集上的分类准确率最高达到了约<strong>0.75</strong>，F1值最高达到约<strong>0.64</strong>，这一结果均发生在约第<strong>7</strong>个Epoch的训练中。</p>
<img src="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/ptamsc_train_summary.png" srcset="/img/loading.gif" lazyload class="" title="ptamsc_train_summary">
<h3 id="结果">结果</h3>
<p>我们实现并采用相同配置训练了上述所有提到的基准模型和主模型，并在验证集上使用各个模型对样本的情感类别标签进行了推理。对于分类效果，我们采用<strong>准确率（Accuracy）</strong>、<strong>精确率（Precision）</strong>、<strong>召回率（Recall）</strong>及<strong>F1</strong>作为评价指标。各个模型在验证集上的分类表现如下表所示：</p>
<table>
<thead>
<tr class="header">
<th>模型</th>
<th>正确率（Accuracy）</th>
<th>精确率（Precision）</th>
<th>召回率（Recall）</th>
<th>F1值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Bert</td>
<td>72.99%</td>
<td>65.56%</td>
<td>54.47%</td>
<td>55.66%</td>
</tr>
<tr class="even">
<td>XLMRoBERTa</td>
<td>73.58%</td>
<td>67.62%</td>
<td>59.48%</td>
<td>61.52%</td>
</tr>
<tr class="odd">
<td>ResNet</td>
<td>57.93%</td>
<td>36.01%</td>
<td>33.71%</td>
<td>25.59%</td>
</tr>
<tr class="even">
<td>Swin Transformer</td>
<td>66.73%</td>
<td>57.90%</td>
<td>54.02%</td>
<td>55.38%</td>
</tr>
<tr class="odd">
<td>X+S(Concatenate)</td>
<td>70.25%</td>
<td>63.74%</td>
<td>54.88%</td>
<td>57.12%</td>
</tr>
<tr class="even">
<td>X+S(Additive)</td>
<td>71.23%</td>
<td>66.31%</td>
<td>54.79%</td>
<td>55.01%</td>
</tr>
<tr class="odd">
<td><strong>PTAMSC</strong></td>
<td><strong>76.51%</strong></td>
<td><strong>70.88%</strong></td>
<td><strong>63.34%</strong></td>
<td><strong>65.69%</strong></td>
</tr>
</tbody>
</table>
<p>可以看到，PTAMSC模型在验证集上的分类正确率达到了<strong>76.51%</strong>，F1值达到了<strong>65.69%</strong>，且在各个指标上均优于所有其他的单模态及多模态基准模型，表明了模型设计的合理性。此外，我们还能发现，对于单模态分类模型，强Baseline的表现均优于基础Baseline，其中XLMRoBERTa相比Bert提升了<strong>10.5%</strong>，Swin Transformer相比ResNet更是提升了<strong>116.4%</strong>。对于多模态模型而言，简单的拼接和加性多模态模型由于其特征无法对齐，分类效果反而出现了一定的下降。</p>
<h2 id="消融试验">消融试验</h2>
<p>接下来我们通过消融实验来探究多模态输入及训练过程中的预训练操作对模型分类表现带来的影响。</p>
<h3 id="单模态输入">单模态输入</h3>
<p>要研究多模态输入对分类结果的影响，我们将输入数据分别改为单文字和单图像输入，并使用训练完成的模型对输入数据进行推理，对比其分类精度。这里我们采用了两种方法实现这一操作：一种是直接取模型中对应模态的推理部分，也即模型分别退化为基准模型中的<strong>XLMRoBERTa</strong>和<strong>Swin Transformer</strong>分类模型；另一种是将其中一个模态的原始数据输入网络，另一模态的数据采用<strong>全0填充</strong>的方式进行替代并输入网络，类似于增加了一个输入掩码。单模态及多模态输入在验证集上的分类结果如下表所示。可以看到，相比退化的分类模型，在原始模型上通过遮盖文字的方式进行单模态输入带来了<strong>0.78%</strong>的正确率提升，而通过遮盖图像的方式进行单模态输入正确率则反而下降了<strong>0.41%</strong>。结合了双模态输入的原始模型分类表现则显著地高于单模态输入的所有模型，这一结果也证明了所设计模型对双模态特征表征融合的有效性。</p>
<table>
<thead>
<tr class="header">
<th>模型</th>
<th>正确率（Accuracy）</th>
<th>精确率（Precision）</th>
<th>召回率（Recall）</th>
<th>F1值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>XLMRoBERTa</td>
<td>73.58%</td>
<td>67.62%</td>
<td>59.48%</td>
<td>61.52%</td>
</tr>
<tr class="even">
<td>Swin Transformer</td>
<td>66.73%</td>
<td>57.90%</td>
<td>54.02%</td>
<td>55.38%</td>
</tr>
<tr class="odd">
<td>PTAMSC（仅文字）</td>
<td>72.99%</td>
<td>63.51%</td>
<td>59.25%</td>
<td>60.77%</td>
</tr>
<tr class="even">
<td>PTAMSC（仅图像）</td>
<td>67.51%</td>
<td>62.89%</td>
<td>59.20%</td>
<td>59.20%</td>
</tr>
<tr class="odd">
<td><strong>PTAMSC</strong></td>
<td><strong>76.51%</strong></td>
<td><strong>70.88%</strong></td>
<td><strong>63.34%</strong></td>
<td><strong>65.69%</strong></td>
</tr>
</tbody>
</table>
<h3 id="预训练带来的提升">预训练带来的提升</h3>
<p>接下来我们继续探究预训练是否给模型带来了分类精度的提升。在同样的训练配置下，我们分别采用<strong>直接在目标数据集上训练</strong>和<strong>在MVSA-Multi数据集上预训练+在目标数据集上微调</strong>的方式重新对模型进行训练，并在验证集上进行分类验证，结果如下表所示。可以看到，经过预训练的模型在验证集上带来了<strong>1.95%</strong>的正确率提升。继续分析后三个指标我们可以发现，预训练主要提升了模型分类的<strong>精确率</strong>，也即模型对于每个类别的<strong>分类置信度</strong>更高。</p>
<table>
<thead>
<tr class="header">
<th>模型</th>
<th>正确率（Accuracy）</th>
<th>精确率（Precision）</th>
<th>召回率（Recall）</th>
<th>F1值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PTAMSC（直接训练）</td>
<td>74.56%</td>
<td>69.04%</td>
<td>63.18%</td>
<td>64.87%</td>
</tr>
<tr class="even">
<td>PTAMSC（预训练+微调）</td>
<td><strong>76.51%</strong></td>
<td><strong>70.88%</strong></td>
<td><strong>63.34%</strong></td>
<td><strong>65.69%</strong></td>
</tr>
</tbody>
</table>
<h2 id="总结">总结</h2>
<p>在本文中，我们对多模态情感分析任务进行了全面的探索和建模。在对不同模态的各种基准模型进行分析和借鉴后，我们从零开始设计并实现了一个基于全Transformer构架的多模态情感分类模型PTAMSC。通过引入预训练和对比学习方法，我们得以显著地提升模型对情感特征的学习和捕捉能力。在目标数据集上，PTAMSC的分类表现全面超过了几种经典的基准模型，且通过消融实验我们可以发现，模型中所使用的几种训练和处理技巧均发挥了作用。最后，我们使用训练完成的模型对目标测试集进行了情感标签推理，结果可见随附的<strong>test_predict.txt</strong>文件。</p>
<h2 id="references">References</h2>
<ol type="1">
<li>Jay Alammar. The illustrated bert, elmo, and co. (how nlp cracked transfer learning). https://jalammar.github.io/illustrated-bert/, 2018.</li>
<li>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine transla- tion by jointly learning to align and translate. CoRR, abs/1409.0473, 2015.</li>
<li>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guil- laume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451, Online, July 2020. Association for Computational Linguistics.</li>
<li>Alexis CONNEAU and Guillaume Lample. Cross-lingual language model pretraining. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.</li>
<li>Feilong Chen, Duzhan Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: A survey on vision-language pre-training. 02 2022.</li>
<li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pretraining of deep bidirectional transformers for language understanding. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.</li>
<li>Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2016.</li>
<li>Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.</li>
<li>Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101, 2017.</li>
<li>Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin transformer v2: Scaling up capacity and resolution. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li>
<li>Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</li>
<li>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.</li>
<li>Zhen Li, Bing Xu, Conghui Zhu, and Tiejun Zhao. CLMLF:a contrastive learning and multi-layer fusion method for multimodal sentiment detection. In Findings of the Association for Computational Linguistics: NAACL 2022. Association for Com- putational Linguistics, 2022.</li>
<li>Yan Ling, Jianfei Yu, and Rui Xia. Vision-language pre-training for multimodal aspect-based sentiment analysis. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2149– 2159, Dublin, Ireland, May 2022. Association for Computational Linguistics.</li>
<li>Teng Niu, Shiai Zhu, Lei Pang, and Abdulmotaleb El Saddik. Sentiment analysis on multi-view social data. In Qi Tian, Nicu Sebe, Guo-Jun Qi, Benoit Huet, Richang Hong, and Xueliang Liu, editors, MultiMedia Modeling, pages 15–27, Cham, 2016. Springer International Publishing.</li>
<li>Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018.</li>
<li>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.</li>
<li>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davi- son, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Can- wen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics.</li>
<li>Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and Xing Xie. Fastformer: Additive attention can be all you need. CoRR, abs/2108.09084, 2021.</li>
<li>Nan Xu and Wenji Mao. Multisentinet: A deep semantic network for multimodal sentiment analysis. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM ’17, page 2399–2402, New York, NY, USA, 2017. Association for Computing Machinery.</li>
<li>Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning. arXiv preprint arXiv:2106.11342, 2021.</li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">当代人工智能</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Computer-Science/">Computer-Science</a>
                    
                      <a class="hover-with-bg" href="/tags/Artificial-Intelligence/">Artificial-Intelligence</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/">
                        <span class="hidden-mobile">当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
