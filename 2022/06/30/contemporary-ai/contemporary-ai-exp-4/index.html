

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="GONGGONGJOHN">
  <meta name="keywords" content="">
  
    <meta name="description" content="摘要 随着Transformer和BERT模型的提出，预训练模型在序列理解及生成任务中的应用正逐渐变得越来越广泛。代码语言理解是近年来从语言理解领域延伸出的一项新兴任务，这一任务的机器学习实现可以大大提高程度开发和调试的效率。在本文中，我们通过对两大NL-PL语言双模态预训练模型CodeBERT和CodeT5的加载和微调，完成了对代码语言理解基准数据集CodeXGLUE的代码摘要生成，并比较了两大">
<meta property="og:type" content="article">
<meta property="og:title" content="当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告">
<meta property="og:url" content="http://gonggongjohn.me/2022/06/30/contemporary-ai/contemporary-ai-exp-4/index.html">
<meta property="og:site_name" content="GONGGONGJOHN&#39;s Blog">
<meta property="og:description" content="摘要 随着Transformer和BERT模型的提出，预训练模型在序列理解及生成任务中的应用正逐渐变得越来越广泛。代码语言理解是近年来从语言理解领域延伸出的一项新兴任务，这一任务的机器学习实现可以大大提高程度开发和调试的效率。在本文中，我们通过对两大NL-PL语言双模态预训练模型CodeBERT和CodeT5的加载和微调，完成了对代码语言理解基准数据集CodeXGLUE的代码摘要生成，并比较了两大">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://gonggongjohn.me/img/contai_project4.png">
<meta property="article:published_time" content="2022-06-30T02:00:00.000Z">
<meta property="article:modified_time" content="2022-12-17T06:03:38.408Z">
<meta property="article:author" content="GONGGONGJOHN">
<meta property="article:tag" content="Computer-Science">
<meta property="article:tag" content="Artificial-Intelligence">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://gonggongjohn.me/img/contai_project4.png">
  
  
  <title>当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告 - GONGGONGJOHN&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"gonggongjohn.me","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="GONGGONGJOHN's Blog" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>GONGGONGJOHN&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-06-30 10:00" pubdate>
        2022年6月30日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      8.1k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      68 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告</h1>
            
            <div class="markdown-body">
              <h2 id="摘要">摘要</h2>
<p>随着Transformer和BERT模型的提出，预训练模型在序列理解及生成任务中的应用正逐渐变得越来越广泛。代码语言理解是近年来从语言理解领域延伸出的一项新兴任务，这一任务的机器学习实现可以大大提高程度开发和调试的效率。在本文中，我们通过对两大NL-PL语言双模态预训练模型CodeBERT和CodeT5的加载和微调，完成了对代码语言理解基准数据集CodeXGLUE的代码摘要生成，并比较了两大模型的结果精度和特性。此外，我们还使用针对Python语言已完成微调的CodeT5模型，对本实验中的代码进行了代码注释生成，并对其生成结果进行了分析。</p>
<p><strong>关键字：自动代码摘要，NL-PL双模态理解，预训练模型，CodeBERT，CodeT5，CodeXGLUE</strong></p>
<h2 id="项目介绍">项目介绍</h2>
<h3 id="任务介绍">任务介绍</h3>
<p><strong>代码摘要（Code Summarization）</strong>是代码语言理解中的一大重要子任务，也是当今绝大多数代码理解和生成数据集中均有包含的一项机器学习算法评测任务。代码摘要的目标是对一段代码所实现的功能进行理解，并使用自然语言的方式给出合适的描述和概括。具体来说，给定一段代码文本序列 <span class="math inline">\(\{s_1, s_2, \cdots, s_n \}\)</span>，模型需要给出一段相应的自然语言文本序列<span class="math inline">\(\{w_1, w_2, \cdots, w_n\}\)</span>，使得这段文本序列能够对代码序列进行表征。</p>
<h3 id="数据集介绍">数据集介绍</h3>
<p><strong>CodeXGLUE</strong>是代码语言理解和生成任务的一大经典数据集，由微软亚洲研究院于2021年公开。CodeXGLUE是一个多任务组合而成的综合评价数据集，其基本情况如下图所示。可以看到，CodeXGLUE共由<strong>10</strong>个子任务组成，其中的数据取自<strong>14</strong>个原有的代码语言理解数据集，可用于评价代码理解模型在各种代码理解和生成相关的下游任务中的综合表现。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codexglue_summary.png" srcset="/img/loading.gif" lazyload class="" title="codexglue_summary">
<p>对于代码摘要任务，CodeXGLUE中使用了其在之前研究中就有涉及过的<strong>CodeSearchNet</strong>数据集。这一数据集是通过对公开的Github仓库代码及其README文件的第一段进行爬取，并通过一系列预处理去除无效和不匹配字符而得到的，其不同语言的样本数量如下图所示。在本实验中，我们使用了其规模最小的<strong>Ruby</strong>数据集作为微调数据集，其包含了<strong>24927</strong>个训练集样本，<strong>1400</strong>个验证集样本以及<strong>1261</strong>个测试集样本。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codesearchnet_summary.png" srcset="/img/loading.gif" lazyload class="" title="codesearchnet_summary">
<h2 id="任务一预训练模型微调">任务一：预训练模型微调</h2>
<h3 id="codebert">CodeBERT</h3>
<h4 id="模型分析">模型分析</h4>
<p><strong>CodeBERT</strong>是<strong>编程-自然语言双模态预训练模型</strong>的开山之作，由微软亚洲研究院联合哈尔滨工业大学、中山大学于2020年提出。受预训练模型Bert的启发，CodeBERT可以通过预训练<strong>对NL-PL的混合语义特征进行通用表示</strong>，并支持如自然语言代码搜索、代码文档生成等一系列多模态下游任务的推理和生成。</p>
<p>CodeBERT采用了和<strong>RoBERTa</strong>完全一样的模型结构，进而又与<strong>Bert</strong>的模型结构一致，因此其基本模型构架如下图所示。可以看到，类Bert模型的基本结构均由多层Transformer编码器组合而成，通过一系列的预训练任务来使得编码器能够根据文本的原始特征输出响应的上下文语义特征。RoBERTa通过对参数的分析优化了Bert的架构和预训练过程，从而进一步挖掘出了Bert的潜能。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/bert_structure.png" srcset="/img/loading.gif" lazyload class="" title="bert_structure">
<p>CodeBERT采用了<strong>NL-PL文本对</strong>的方式作为预训练阶段模型的输入表示，并使用分隔符进行标记。具体来说，模型的输入为 <span class="math inline">\([CLS], w_1, w_2, \cdots, w_n, [SEP], c_1, c_2, \cdots, c_m, [EOS]\)</span>，其中 <span class="math inline">\([CLS]\)</span> 为句体标识符，<span class="math inline">\([SEP]\)</span> 为NL/PL分隔符，<span class="math inline">\([EOS]\)</span> 为句体结束表示符，这种表示也是类Bert模型常用的文本对表示方式。</p>
<p>CodeBERT的预训练任务分为两个，其中一个是经典的<strong>遮盖语言建模（Masked Language Modeling）</strong>任务，也即随机地遮盖住NL-PL对中的某个部分，并让模型预测遮盖住的部分。另一个任务为<strong>词项替换检测（Replaced Token Detection）</strong>任务，其原理如下图所示。可以看到，RTD任务采用了一个类似<strong>GAN</strong>的结构，在前一个任务的基础上，通过两个生成器分别对遮盖住的NL词项和PL词项进行生成，并使用一个判别器对生成的词项进行替换检测来判断生成的词项是否为原来的词项。通过这种对抗学习的思想，预训练过程可以同时使用<strong>双模态（NL-PL）</strong>和<strong>单模态（PL）</strong>的语料对模型进行训练，从而提升模型的表征效果。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codebert_rtd.png" srcset="/img/loading.gif" lazyload class="" title="codebert_rtd">
<h4 id="微调实践">微调实践</h4>
<p>可以看到，CodeBERT本质上是一个语义编码模型，其本身并不具有文本生成的功能。要使用其来进行代码摘要任务，我们需要额外设计一个<strong>解码器（Decoder）</strong>来进行摘要文本生成。由于CodeBERT的编码器部分全部由Transformer结构组成，为了保持统一，在解码器部分，我们同样采用多层Transformer块的方式对文本进行解码。在CodeBERT的原始论文中，作者在代码摘要任务的评估中使用了<strong>6</strong>层Transformer作为解码器，其中包含<strong>12</strong>个注意力头和<strong>768</strong>维的隐藏层，这里我们<strong>保持这一原有设置进行实现</strong>。加入解码器后，整个代码摘要模型生成流程如下图所示。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codebert_codesum_structure.png" srcset="/img/loading.gif" lazyload class="" title="codebert_codesum_structure">
<p>我们使用<strong>Pytorch</strong>和<strong>transformers</strong>库对上面的结构进行了实现，部分代码参考了CodeBERT的原始代码实现。代码的具体细节见报告随附代码中的<strong>codebert_model.py</strong>。我们使用同样为Huggingface实现的<strong>datasets</strong>模块对CodeXGLUE数据集进行了加载，并将其作为微调任务中的训练、验证和测试数据集。由于其已经划分好了训练、验证和测试部分，我们只需要逐一加载即可。对于预训练模型，我们使用了微软官方提供的<strong>microsoft/codebert-base</strong>模型。为了复现原论文中的结果，我们将微调的超参数设置与原文中保持一致，也即将学习率设为<strong>5e-5</strong>，<strong>Batch Size</strong>设置为<strong>16</strong>，共训练<strong>8</strong>个Epochs（<strong>12464</strong>个Steps）。训练时Loss随迭代次数的变化情况如下图所示。可以看到，训练Loss在前<strong>2000</strong>代时快速下降，随后每隔约<strong>1000</strong>代出现一次波动并逐步下降，这与<strong>小批量梯度下降算法</strong>的Loss特征相吻合。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codebert_loss.png" srcset="/img/loading.gif" lazyload class="" title="codebert_loss">
<p>在每一代训练完成后，我们使用目标数据集中的验证集对模型的生成结果进行评估。我们使用<strong>BLEU-4（Bilingual Evaluation Understudy-4 Gram）</strong>和<strong>困惑度（Perplexity）</strong>作为生成结果的评价指标。结果如下图所示。可以看到，模型在验证集上BLEU-4的最好表现达到了<strong>13.26</strong>，出现在<strong>Epoch=7</strong>时；困惑度最低约为<strong>61.40</strong>，出现在<strong>Epoch=4</strong>的时候。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codebert_bleu_ppl.png" srcset="/img/loading.gif" lazyload class="" title="codebert_bleu_ppl">
<p>训练完成后，我们对测试集中的代码进行摘要文本生成，其BLEU-4结果如下表所示。可以看到，本次训练的模型在相同测试集上的生成结果与原论文的差距约为 <span class="math inline">\(\boldsymbol{0.7\%}\)</span>，考虑到初始化的随机性，该误差可接受。表明这一模型实现和训练过程与原论文相符。</p>
<table>
<thead>
<tr class="header">
<th>模型</th>
<th>测试集预测结果</th>
<th>原始论文结果</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CodeBERT+Transformer</td>
<td><strong>12.07</strong></td>
<td>12.16</td>
</tr>
</tbody>
</table>
<h3 id="codet5">CodeT5</h3>
<h4 id="模型分析-1">模型分析</h4>
<p><strong>CodeT5</strong>是一个<strong>端到端的NL-PL序列生成类预训练模型</strong>，由Salesforce亚洲研究院于2021年提出。CodeT5旨在解决此前模型仅对编码器或解码器进行预训练从而导致其在序列生成类任务上训练不连贯的问题，该模型希望通过一个完整的端到端预训练模型来提升序列生成的精度。</p>
<p>CodeT5基于机器翻译模型<strong>T5</strong>演化而来，其模型结构也与T5一致。而T5模型采用了一个标准的<strong>Transformer Encoder-Decoder</strong>构架，因此其模型结构如下图所示。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codet5_architecture.png" srcset="/img/loading.gif" lazyload class="" title="codet5_architecture">
<p>与CodeBERT类似，CodeT5同样采用了NL-PL文本对结合特殊分割符的方式作为预训练阶段模型的输入表示。此外，为了捕获更多代码语言中的特征，CodeT5还通过将代码转化为<strong>抽象语法树（Abstract Syntax Tree）</strong>的方式来标记其<strong>函数、变量标识符</strong>的位置，并作为<strong>片段嵌入层（Segment Embedding）</strong>输入模型。</p>
<p>CodeT5的预训练流程如下图所示。具体来说，其共有如下4个预训练任务：<strong>遮盖片段预测（Masked Span Prediction）</strong>、<strong>标识符标记（Identifier Tagging）</strong>、<strong>遮盖标识符预测（Masked Identifier Prediction）</strong>和<strong>双模态双向生成（Bimodal Dual Generation）</strong>。其中，MSP是一种经典的类Seq2Seq模型预训练任务，用于建立NL-PL转换的通用表征；IT和MIP任务用于学习代码语言特有的关键字和标识符信息；BDG任务本身即为双向生成预训练，可以有效消除预训练过程和下游任务训练中的任务差异。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codet5_pretrain_procedure.png" srcset="/img/loading.gif" lazyload class="" title="codet5_pretrain_procedure">
<h4 id="微调实践-1">微调实践</h4>
<p>由于CodeT5本身即为一个端到端的生成模型，因此我们无需增加额外的模块。Salesforce官方提供了一份CodeT5在各种下游任务中的实现代码，这里我们直接使用其中的代码摘要模块来完成对CodeXGLUE-Ruby数据集的微调任务。对于预训练模型，我们使用了其在HuggingFace上提供的<strong>Salesforce/codet5-base</strong>模型，并使用原论文中的超参数设置对目标数据集进行了微调。训练时Loss随迭代次数的变化情况如下图所示。可以看到，训练Loss在前<strong>500</strong>代时快速下降，随后每隔约<strong>500</strong>代出现一次波动并逐步下降，由于这一次训练的Batch Size更大（<strong>Batch Size=48</strong>），因此这一曲线符合小批量梯度下降的Loss特征，表明模型收敛有效。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codet5_loss.png" srcset="/img/loading.gif" lazyload class="" title="codet5_loss">
<p>与CodeBERT类似，在每一代训练完成后，我们使用目标数据集中的验证集对模型的生成结果进行评估，并同样使用<strong>BLEU-4</strong>和<strong>困惑度</strong>作为评价指标。结果如下图所示。可以看到，模型在验证集上BLEU-4的最好表现约为<strong>16.71</strong>，出现在<strong>Epoch=2</strong>时；困惑度最低约为<strong>1.423</strong>，出现在<strong>Epoch=2</strong>的时候。由于该模型本身即为一个端到端的模型，且预训练任务中已经有对文本生成的预训练，因此其收敛较快，多次训练后反而出现了效果下降的情况。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codet5_bleu_ppl.png" srcset="/img/loading.gif" lazyload class="" title="codet5_bleu_ppl">
<p>训练完成后，我们对测试集中的代码进行摘要文本生成，其BLEU-4结果如下表所示。可以看到，本次训练的模型在相同测试集上的生成结果与原论文的差距约为 <span class="math inline">\(\boldsymbol{0.6\%}\)</span>，考虑误差因素，该结果可被接受。此外，我们可以看到，相比CodeBERT+Transformer的代码摘要模型，CodeT5在Ruby数据集上的BLEU-4结果提升了约<span class="math inline">\(\boldsymbol{29.58\%}\)</span>，这一提升时十分可观的。</p>
<table>
<thead>
<tr class="header">
<th>模型</th>
<th>测试集预测结果</th>
<th>原始论文结果</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CodeT5</td>
<td><strong>15.64</strong></td>
<td>15.73</td>
</tr>
</tbody>
</table>
<h2 id="任务二python函数注释">任务二：Python函数注释</h2>
<p>接下来我们使用经过微调的CodeT5对本次实验的代码进行自动注释。CodeT5官方提供了一个已经完成微调的代码摘要模型<strong>检查点（Checkpoint）</strong>，这一检查点可以支持多个语言的代码摘要生成，我们直接使用transformers库的<strong>T5ForConditionalGeneration.from_pretrained()</strong>方法即可加载这一模型。</p>
<p>我们选取CodeBERT和CodeT5中实现代码中函数的部分（codet5/data/summarize/source.txt），并将其预处理为统一的格式以方便读取，预处理代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><br>instance_list = []<br><span class="hljs-keyword">with</span> open(<span class="hljs-string">&#x27;data/summarize/python/source.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    line = f.readline()<br>    code_tmp = <span class="hljs-string">&#x27;&#x27;</span><br>    cnt = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> line:<br>        <span class="hljs-keyword">if</span> line == <span class="hljs-string">&#x27;[SEP]\n&#x27;</span>:<br>            cnt += <span class="hljs-number">1</span><br>            instance_list.append(&#123;<span class="hljs-string">&#x27;id&#x27;</span>: cnt, <span class="hljs-string">&#x27;code&#x27;</span>: code_tmp&#125;)<br>            code_tmp = <span class="hljs-string">&#x27;&#x27;</span><br>        <span class="hljs-keyword">else</span>:<br>            code_tmp += line<br>        line = f.readline()<br><span class="hljs-keyword">with</span> open(<span class="hljs-string">&#x27;data/summarize/python/source.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f_out:<br>    json.dump(instance_list, f_out)<br></code></pre></td></tr></table></figure>
<p>处理完成的代码文本见随附代码的<strong>codet5/data/summarize/source.json</strong>。随后，我们使用字符串方式将其逐行读入，并输入CodeT5模型中进行函数注释生成，结果如下图所示。可以看到，对于一些功能相对清晰的函数，CodeT5可以准确地概括其代码意图，甚至对于一些额外库中的对象（如Pandas中的Dataframe），模型也可以准确地生成其语义。然而，对于一些需要整体功能背景的函数段来说，CodeT5的摘要结果与其实际作用便存在着一些差距，如下图所示。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codet5_inference.png" srcset="/img/loading.gif" lazyload class="" title="codet5_inference">
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codet5_inference_compare.png" srcset="/img/loading.gif" lazyload class="" title="codet5_inference_compare">
<p>完整生成结果可见<strong>codet5/data/summarize/summary.json</strong>。</p>
<h2 id="总结">总结</h2>
<p>在本文中，我们对预训练模型在代码摘要任务上的训练和推理过程进行了较为全面的探索。通过分别使用经过预训练的CodeBERT和CodeT5模型在CodeXGLUE数据集上进行微调并对比其生成结果，我们能够较好地看出近年来预训练模型在NL-PL双模态理解和生成任务上的发展。随后，我们使用经过预训练的CodeT5模型对实验代码进行实际的函数注释生成，通过对结果的分析，我们能够看出机器学习模型在该任务上的推理表现和特性。</p>
<h2 id="references">References</h2>
<ol type="1">
<li>Jay Alammar. The illustrated transformer. http://jalammar.github.io/ illustrated-transformer/, 2018.</li>
<li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Com- putational Linguistics.</li>
<li>Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre- trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1536–1547, Online, November 2020. Association for Computational Linguistics.</li>
<li>Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A ma- chine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664, 2021.</li>
<li>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.</li>
<li>Microsoft. Codebert. https://github.com/microsoft/CodeBERT, 2021.</li>
<li>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learn- ing with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.</li>
<li>Salesforce. Codet5. https://github.com/salesforce/CodeT5, 2021.</li>
<li>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics.</li>
<li>Yungao Xie, Hong Wen, and Qing Yang. Ternary sentiment classification of air- line passengers’twitter text based on bert. Journal of Physics: Conference Series, 1813:012017, 02 2021.</li>
<li>Shafiq Joty Steven C.H. Hoi Yue Wang, Weishi Wang. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, 2021.</li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">当代人工智能</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Computer-Science/">Computer-Science</a>
                    
                      <a class="hover-with-bg" href="/tags/Artificial-Intelligence/">Artificial-Intelligence</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">当代人工智能 课程项目五 多模态情感分析实验报告</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/">
                        <span class="hidden-mobile">当代人工智能 课程项目三 图像分类及经典CNN实现</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
