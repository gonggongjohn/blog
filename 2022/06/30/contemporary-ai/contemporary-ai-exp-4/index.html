

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="GONGGONGJOHN">
  <meta name="keywords" content="">
  
    <meta name="description" content="摘要 随着Transformer和BERT模型的提出，预训练模型在序列理解及生成任务中的应用正逐渐变得越来越广泛。代码语言理解是近年来从语言理解领域延伸出的一项新兴任务，这一任务的机器学习实现可以大大提高程度开发和调试的效率。在本文中，我们通过对两大NL-PL语言双模态预训练模型CodeBERT和CodeT5的加载和微调，完成了对代码语言理解基准数据集CodeXGLUE的代码摘要生成，并比较了两大">
<meta property="og:type" content="article">
<meta property="og:title" content="当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告">
<meta property="og:url" content="http://gonggongjohn.me/2022/06/30/contemporary-ai/contemporary-ai-exp-4/index.html">
<meta property="og:site_name" content="GONGGONGJOHN&#39;s Blog">
<meta property="og:description" content="摘要 随着Transformer和BERT模型的提出，预训练模型在序列理解及生成任务中的应用正逐渐变得越来越广泛。代码语言理解是近年来从语言理解领域延伸出的一项新兴任务，这一任务的机器学习实现可以大大提高程度开发和调试的效率。在本文中，我们通过对两大NL-PL语言双模态预训练模型CodeBERT和CodeT5的加载和微调，完成了对代码语言理解基准数据集CodeXGLUE的代码摘要生成，并比较了两大">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://gonggongjohn.me/img/contai_project4.png">
<meta property="article:published_time" content="2022-06-30T02:00:00.000Z">
<meta property="article:modified_time" content="2022-12-16T09:12:41.271Z">
<meta property="article:author" content="GONGGONGJOHN">
<meta property="article:tag" content="Computer-Science">
<meta property="article:tag" content="Artificial-Intelligence">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://gonggongjohn.me/img/contai_project4.png">
  
  
  <title>当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告 - GONGGONGJOHN&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"gonggongjohn.me","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="GONGGONGJOHN's Blog" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>GONGGONGJOHN&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-06-30 10:00" pubdate>
        2022年6月30日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.8k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      40 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告</h1>
            
            <div class="markdown-body">
              <h2 id="摘要">摘要</h2>
<p>随着Transformer和BERT模型的提出，预训练模型在序列理解及生成任务中的应用正逐渐变得越来越广泛。代码语言理解是近年来从语言理解领域延伸出的一项新兴任务，这一任务的机器学习实现可以大大提高程度开发和调试的效率。在本文中，我们通过对两大NL-PL语言双模态预训练模型CodeBERT和CodeT5的加载和微调，完成了对代码语言理解基准数据集CodeXGLUE的代码摘要生成，并比较了两大模型的结果精度和特性。此外，我们还使用针对Python语言已完成微调的CodeT5模型，对本实验中的代码进行了代码注释生成，并对其生成结果进行了分析。</p>
<p><strong>关键字：自动代码摘要，NL-PL双模态理解，预训练模型，CodeBERT，CodeT5，CodeXGLUE</strong></p>
<h2 id="项目介绍">项目介绍</h2>
<h3 id="任务介绍">任务介绍</h3>
<p><strong>代码摘要（Code Summarization）</strong>是代码语言理解中的一大重要子任务，也是当今绝大多数代码理解和生成数据集中均有包含的一项机器学习算法评测任务。代码摘要的目标是对一段代码所实现的功能进行理解，并使用自然语言的方式给出合适的描述和概括。具体来说，给定一段代码文本序列 <span class="math inline">\(\{s_1, s_2, \cdots, s_n \}\)</span>，模型需要给出一段相应的自然语言文本序列<span class="math inline">\(\{w_1, w_2, \cdots, w_n\}\)</span>，使得这段文本序列能够对代码序列进行表征。</p>
<h3 id="数据集介绍">数据集介绍</h3>
<p><strong>CodeXGLUE</strong>是代码语言理解和生成任务的一大经典数据集，由微软亚洲研究院于2021年公开。CodeXGLUE是一个多任务组合而成的综合评价数据集，其基本情况如下图所示。可以看到，CodeXGLUE共由<strong>10</strong>个子任务组成，其中的数据取自<strong>14</strong>个原有的代码语言理解数据集，可用于评价代码理解模型在各种代码理解和生成相关的下游任务中的综合表现。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codexglue_summary.png" srcset="/img/loading.gif" lazyload class="" title="codexglue_summary">
<p>对于代码摘要任务，CodeXGLUE中使用了其在之前研究中就有涉及过的<strong>CodeSearchNet</strong>数据集。这一数据集是通过对公开的Github仓库代码及其README文件的第一段进行爬取，并通过一系列预处理去除无效和不匹配字符而得到的，其不同语言的样本数量如下图所示。在本实验中，我们使用了其规模最小的<strong>Ruby</strong>数据集作为微调数据集，其包含了<strong>24927</strong>个训练集样本，<strong>1400</strong>个验证集样本以及<strong>1261</strong>个测试集样本。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codesearchnet_summary.png" srcset="/img/loading.gif" lazyload class="" title="codesearchnet_summary">
<h2 id="任务一预训练模型微调">任务一：预训练模型微调</h2>
<h3 id="codebert">CodeBERT</h3>
<h4 id="模型分析">模型分析</h4>
<h4 id="微调实践">微调实践</h4>
<h3 id="codet5">CodeT5</h3>
<h4 id="模型分析-1">模型分析</h4>
<h4 id="微调实践-1">微调实践</h4>
<h2 id="任务二python函数注释">任务二：Python函数注释</h2>
<p>接下来我们使用经过微调的CodeT5对本次实验的代码进行自动注释。CodeT5官方提供了一个已经完成微调的代码摘要模型<strong>检查点（Checkpoint）</strong>，这一检查点可以支持多个语言的代码摘要生成，我们直接使用transformers库的<strong>T5ForConditionalGeneration.from_pretrained()</strong>方法即可加载这一模型。</p>
<p>我们选取CodeBERT和CodeT5中实现代码中函数的部分（codet5/data/summarize/source.txt），并将其预处理为统一的格式以方便读取，预处理代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><br>instance_list = []<br><span class="hljs-keyword">with</span> open(<span class="hljs-string">&#x27;data/summarize/python/source.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    line = f.readline()<br>    code_tmp = <span class="hljs-string">&#x27;&#x27;</span><br>    cnt = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> line:<br>        <span class="hljs-keyword">if</span> line == <span class="hljs-string">&#x27;[SEP]\n&#x27;</span>:<br>            cnt += <span class="hljs-number">1</span><br>            instance_list.append(&#123;<span class="hljs-string">&#x27;id&#x27;</span>: cnt, <span class="hljs-string">&#x27;code&#x27;</span>: code_tmp&#125;)<br>            code_tmp = <span class="hljs-string">&#x27;&#x27;</span><br>        <span class="hljs-keyword">else</span>:<br>            code_tmp += line<br>        line = f.readline()<br><span class="hljs-keyword">with</span> open(<span class="hljs-string">&#x27;data/summarize/python/source.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f_out:<br>    json.dump(instance_list, f_out)<br></code></pre></td></tr></table></figure>
<p>处理完成的代码文本见随附代码的<strong>codet5/data/summarize/source.json</strong>。随后，我们使用字符串方式将其逐行读入，并输入CodeT5模型中进行函数注释生成，结果如下图所示。可以看到，对于一些功能相对清晰的函数，CodeT5可以准确地概括其代码意图，甚至对于一些额外库中的对象（如Pandas中的Dataframe），模型也可以准确地生成其语义。然而，对于一些需要整体功能背景的函数段来说，CodeT5的摘要结果与其实际作用便存在着一些差距，如下图所示。</p>
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codet5_inference.png" srcset="/img/loading.gif" lazyload class="" title="codet5_inference">
<img src="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/codet5_inference_compare.png" srcset="/img/loading.gif" lazyload class="" title="codet5_inference_compare">
<p>完整生成结果可见<strong>codet5/data/summarize/summary.json</strong>。</p>
<h2 id="总结">总结</h2>
<p>在本文中，我们对预训练模型在代码摘要任务上的训练和推理过程进行了较为全面的探索。通过分别使用经过预训练的CodeBERT和CodeT5模型在CodeXGLUE数据集上进行微调并对比其生成结果，我们能够较好地看出近年来预训练模型在NL-PL双模态理解和生成任务上的发展。随后，我们使用经过预训练的CodeT5模型对实验代码进行实际的函数注释生成，通过对结果的分析，我们能够看出机器学习模型在该任务上的推理表现和特性。</p>
<h2 id="references">References</h2>
<ol type="1">
<li>Jay Alammar. The illustrated transformer. http://jalammar.github.io/ illustrated-transformer/, 2018.</li>
<li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Com- putational Linguistics.</li>
<li>Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. CodeBERT: A pre- trained model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1536–1547, Online, November 2020. Association for Computational Linguistics.</li>
<li>Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. Codexglue: A ma- chine learning benchmark dataset for code understanding and generation. CoRR, abs/2102.04664, 2021.</li>
<li>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019.</li>
<li>Microsoft. Codebert. https://github.com/microsoft/CodeBERT, 2021.</li>
<li>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learn- ing with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.</li>
<li>Salesforce. Codet5. https://github.com/salesforce/CodeT5, 2021.</li>
<li>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics.</li>
<li>Yungao Xie, Hong Wen, and Qing Yang. Ternary sentiment classification of air- line passengers’twitter text based on bert. Journal of Physics: Conference Series, 1813:012017, 02 2021.</li>
<li>Shafiq Joty Steven C.H. Hoi Yue Wang, Weishi Wang. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, 2021.</li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">当代人工智能</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Computer-Science/">Computer-Science</a>
                    
                      <a class="hover-with-bg" href="/tags/Artificial-Intelligence/">Artificial-Intelligence</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/07/05/contemporary-ai/contemporary-ai-exp-5/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">当代人工智能 课程项目五 多模态情感分析实验报告</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/">
                        <span class="hidden-mobile">当代人工智能 课程项目三 图像分类及经典CNN实现</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
