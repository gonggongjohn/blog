

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="GONGGONGJOHN">
  <meta name="keywords" content="">
  
    <meta name="description" content="摘要 在本文中，我们对卷积神经网络在图像分类中的发展进行了全面的梳理和探索。通过实现六大经典神经网络构架：LeNet、AlexNet、VGGNet、ResNet、MobileNet 和 ConvNeXt，并将其应用于 MNIST 数据集的分类任务中，我们可以看到不同网络构架和设计思想对分类结果的影响。此外，我们还通过硬件消耗和资源占用情况分析了不同网络构架的效能，讨论了不同网络结构的适用场景。 关">
<meta property="og:type" content="article">
<meta property="og:title" content="当代人工智能 课程项目三 图像分类及经典CNN实现">
<meta property="og:url" content="http://gonggongjohn.me/2022/05/02/contemporary-ai/contemporary-ai-exp-3/index.html">
<meta property="og:site_name" content="GONGGONGJOHN&#39;s Blog">
<meta property="og:description" content="摘要 在本文中，我们对卷积神经网络在图像分类中的发展进行了全面的梳理和探索。通过实现六大经典神经网络构架：LeNet、AlexNet、VGGNet、ResNet、MobileNet 和 ConvNeXt，并将其应用于 MNIST 数据集的分类任务中，我们可以看到不同网络构架和设计思想对分类结果的影响。此外，我们还通过硬件消耗和资源占用情况分析了不同网络构架的效能，讨论了不同网络结构的适用场景。 关">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://gonggongjohn.me/img/contai_project3.png">
<meta property="article:published_time" content="2022-05-02T02:00:00.000Z">
<meta property="article:modified_time" content="2022-12-16T09:04:00.241Z">
<meta property="article:author" content="GONGGONGJOHN">
<meta property="article:tag" content="Computer-Science">
<meta property="article:tag" content="Artificial-Intelligence">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://gonggongjohn.me/img/contai_project3.png">
  
  
  <title>当代人工智能 课程项目三 图像分类及经典CNN实现 - GONGGONGJOHN&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"gonggongjohn.me","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="GONGGONGJOHN's Blog" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>GONGGONGJOHN&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="当代人工智能 课程项目三 图像分类及经典CNN实现">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-05-02 10:00" pubdate>
        2022年5月2日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      18k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      153 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">当代人工智能 课程项目三 图像分类及经典CNN实现</h1>
            
            <div class="markdown-body">
              <h2 id="摘要">摘要</h2>
<p>在本文中，我们对卷积神经网络在图像分类中的发展进行了全面的梳理和探索。通过实现六大经典神经网络构架：LeNet、AlexNet、VGGNet、ResNet、MobileNet 和 ConvNeXt，并将其应用于 MNIST 数据集的分类任务中，我们可以看到不同网络构架和设计思想对分类结果的影响。此外，我们还通过硬件消耗和资源占用情况分析了不同网络构架的效能，讨论了不同网络结构的适用场景。</p>
<p><strong>关键字：图像分类，MNIST，LeNet，AlexNet，VGGNet，ResNet，MobileNet，ConvNeXt</strong></p>
<h2 id="项目介绍">项目介绍</h2>
<h3 id="任务介绍">任务介绍</h3>
<p>计算机视觉作为人工智能和计算机科学的一大重要领域，已经逐渐在越来越多的应用场景下发挥重要的作用。在计算机视觉中，图像分类是一项十分经典且基础的任务，其是很多下游任务（如语义分割、目标检测等）的前置任务。早期的图像分类主要使用特征工程方法，依赖人工设计的图像特征对图像进行判别。随着计算机算力和神经网络技术的发展，基于深度学习的图像分类方法在近十几年得到了前所未有的发展。</p>
<p>图像分类任务的具体定义如下：给定一组图像的计算机表示 <span class="math inline">\(\mathcal{I} = \{x_1, x_2, \cdots, x_n\}\)</span> 和类别集 <span class="math inline">\(\mathcal{C} = \{c_1, c_2, · · · , c_k\}\)</span>，我们需要让机器学习出一种映射 <span class="math inline">\(f : \mathcal{I} \to \mathcal{C}\)</span>，使得对于任意 <span class="math inline">\(x \in \mathcal{I}\)</span>，存在一 个<span class="math inline">\(c \in \mathcal{C}\)</span>，使得 <span class="math inline">\(f(x) = c\)</span>。</p>
<p>在本项目中，我们需要复现和对比近年来几大经典的基于<strong>卷积神经网络（Convolutional Neu- ral Network）</strong>的深度学习构架，实现对图像的多分类任务。</p>
<h3 id="数据集介绍">数据集介绍</h3>
<p><strong>MNIST（Modified NIST）</strong>数据集是一个手写体数字图像数据集，由 Yann LeCun 等人首先提出并用于验证其于 1989 年提出的卷积神经网络分类器。该数据集来自于由<strong>美国国家标准与技术研究所（National Institute of Standards and Technology）</strong>发起整理的<strong>Special Database 3</strong>和<strong>Special Database 1</strong>手写数字图像数据库，其中前者来自于高中生，后者来自于人口普查局的工作人员。</p>
<p>MNIST 数据集共包含<strong>70000</strong>张图像，其中训练集含有<strong>60000</strong>张图像，测试集含有<strong>10000</strong>张图像。MNIST 数据集中的图像为单通道黑白图像，图像尺寸为 <span class="math inline">\(28 \times 28\)</span>，其中手写数字被规范化到了中心的 <span class="math inline">\(20 \times 20\)</span> 范围内。数据集中的部分图像如下图所示。可以看到，该数据集在预处理过程中已经去除了大部分图像的噪声，因此在模式识别早期，其是一个很好的验证模型特征提取能力的数据集。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/mnist_visualize.png" srcset="/img/loading.gif" lazyload class="" title="mnist_visualize">
<p>由于 MNIST 数据集十分经典，许多现代神经网络推理框架中已经集成了该数据集，并将数据集中的内容与框架中的数据结构进行了统一，这为我们省去了许多读取和预处理的麻烦。在本项目中，我们使用 PyTorch 框架附带的<strong>TorchVision</strong>视觉图形库直接加载这一数据集，并将其数据格式转化为 PyTorch 中的<strong>张量（Tensor）</strong>格式，实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision.datasets <span class="hljs-keyword">import</span> MNIST<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><br>train_data = MNIST(<span class="hljs-string">&#x27;mnist&#x27;</span>, train=<span class="hljs-literal">True</span>, transform=ToTensor(), download=<span class="hljs-literal">True</span>) test_data = MNIST(<span class="hljs-string">&#x27;mnist&#x27;</span>, train=<span class="hljs-literal">False</span>, transform=ToTensor(), download=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<h2 id="模型">模型</h2>
<h3 id="lenet">LeNet</h3>
<h4 id="模型介绍">模型介绍</h4>
<p><strong>LeNet</strong>是由<strong>Yann LeCun</strong>等人于 1989 年提出的一种卷积神经网络构架，其被认为是现代卷积神经网络的开山之作。在早期模式识别时代，绝大多数的模式识别系统都是靠人工设计的特征或是人工特征+自动学习算法的方式来实现的。而由于自然数据的多样式，无论是语音、字形还是其他类型的模式，几乎不可能完全靠手工来建立一个精确的识别系统。LeCun 等人认为，通过巧妙的对自动学习系统的构造，模式识别系统可以完全依赖自动学习技术，而不是手工设计的启发式方法。以字符识别为例，LeCun 等人在 1998 年的论文中证明人工设计特征抽取的工作可以通过特别设计的机器学习方法替代，并直接应用在原始的像素图像上。</p>
<p>LeNet 自提出以来有多个适用于不同任务的网络结构变种，现在我们所说的 LeNet 网络结构一般指 LeCun 等人在 1998 年论文中提出的 LeNet-5 网络。LeNet-5 的整体模型构架如下图所示：</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/lenet_architecture.png" srcset="/img/loading.gif" lazyload class="" title="lenet_architecture">
<p>可以看到，整个 LeNet-5 网络共有<strong>7</strong>层，包含<strong>卷积层</strong>、<strong>池化层</strong>和<strong>全连接层</strong>。<strong>卷积层</strong>的卷积核 大小统一为 5 × 5，步长统一为<strong>1</strong>，其中第一个卷积层（C1）有<strong>6</strong>个卷积核，第二个卷积层（C3）有<strong>16</strong>个卷积核，第三个卷积层（C5）有<strong>120</strong>个卷积核。<strong>池化层</strong>的池化核大小统一为 <span class="math inline">\(2 \times 2\)</span>，池化方式统一为<strong>平均池化（Average Pooling）</strong>。在经过最后一个卷积层后，输入数据变为一个 <span class="math inline">\(120 \times 1\)</span> 的特征向量，再经过一个<strong>84</strong>维的<strong>全连接层</strong>后连接到<strong>10</strong>维的<strong>输出层</strong>上。</p>
<p>由于LeNet提出时间较早，其网络结构的部分细节与现代常见的卷积神经网络结构并不相同。其中最为显著的便是<strong>第一池化层（S2）</strong>和<strong>第二卷积层（C3）</strong>之间的<strong>不完全连接</strong>特性。具体来说，为了使 C3 层的不同卷积核之间能够表示不同的图像特征，S2 层和 C3 之间并不采用完全连 接的方式，而是人为地将部分单元相连接，具体连接方式如下图所示。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/lenet_partial_connection.png" srcset="/img/loading.gif" lazyload class="" title="lenet_partial_connection">
<p>此外，与现代网络结构中不同，LeNet 中的池化层除了对上一层的结果进行平均池化操作外，还需要通过一个线性变换以及 Sigmoid 激活函数。具体来说，池化层的计算公式如下： <span class="math display">\[
\mathrm{AvgPool}(ch, i) = \sigma \left( w_{ch} \cdot \frac{1}{4} \sum_{j = 1}^4 \mathrm{input}_j + b_{ch} \right)
\]</span> 其中 <span class="math inline">\(w_{ch}\)</span> 和 <span class="math inline">\(b_{ch}\)</span> 为可训练参数（每个池化核各对应一个）。</p>
<h4 id="模型实现">模型实现</h4>
<p>下面我们使用<strong>Pytorch</strong>框架来实现LeNet-5网络构架。从上面的介绍中我们知道，LeNet-5在数据经过池化层后还需要通过一个<strong>线性变换+Sigmoid激活层</strong>的结构，这一操作在现代的卷积神经网络构架中是不常见的，在Pytorch中并没有相关的封装类，因此我们首先来实现这一结构（依通道作线形变换），具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ChannelWiseLinear2d</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, channel</span>):</span><br>        super(ChannelWiseLinear2d, self).__init__()<br>        self.w = nn.Parameter(torch.rand(channel))<br>        self.b = nn.Parameter(torch.rand(channel))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        bs, ch, w, h = x.shape<br>        w_full = torch.zeros(bs, ch, w, h)<br>        b_full = torch.zeros(bs, ch, w, h)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(ch):<br>            w_full[:, i, :, :] = torch.full((bs, w, h), self.w[i].item())<br>            b_full[:, i, :, :] = torch.full((bs, w, h), self.b[i].item())<br>        <span class="hljs-keyword">return</span> w_full * x + b_full<br></code></pre></td></tr></table></figure>
<p>根据上面的介绍我们还可以知道，LeNet-5中的S2和C3层之间采用了<strong>不完全连接</strong>的方式进行连接。若要使用Pytorch的卷积层来实现这一效果，一个可行的办法是手动构造一个<strong>遮盖矩阵（Mask Matrix）</strong>，将遮盖住的连接参数置为0并加以冻结，以阻止输入数据传入未被连接的单元。通过阅读LeNet的原始论文我们可以知道，这一处理主要是为了迫使不同的卷积核对输入学习出不同的特征以防止所有的卷积核收敛至相同的参数，若用论文中的原话，就是<strong>打破网络结构中的对称性（break of symmetry in the network）</strong>。在更为现代的神经网络结构中，我们通常会通过<strong>随机初始化（Random Initialization）</strong>的方法来打破这一对称性，并且可以证明，这一方法相比手动地连接不同层之间的单元更为有效。由于Pytorch默认使用随机方法初始化网络参数，因此我们不再需要使用LeNet原文中的不完全连接操作。</p>
<p>有了上面的准备之后，现在我们可以来实现完整的LeNet-5结构了，具体实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LeNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        super(LeNet, self).__init__()<br>        self.extractor = nn.Sequential(<br>            nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>            ChannelWiseLinear2d(channel=<span class="hljs-number">6</span>),<br>            nn.Sigmoid(),<br>            nn.Conv2d(in_channels=<span class="hljs-number">6</span>, out_channels=<span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>            ChannelWiseLinear2d(channel=<span class="hljs-number">16</span>),<br>            nn.Sigmoid(),<br>            nn.Conv2d(in_channels=<span class="hljs-number">16</span>, out_channels=<span class="hljs-number">120</span>, kernel_size=<span class="hljs-number">5</span>)<br>        )<br>        self.dense = nn.Sequential(<br>            nn.Linear(in_features=<span class="hljs-number">120</span>, out_features=<span class="hljs-number">84</span>),<br>            nn.Linear(in_features=<span class="hljs-number">84</span>, out_features=<span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        feature = self.extractor(x)<br>        feature = feature.view(feature.size(<span class="hljs-number">0</span>), <span class="hljs-number">-1</span>)<br>        out = self.dense(feature)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>
<p>事实上，在原始的LeNet-5网络结构中，输出层中的单元并不是普通的输出单元，而是由<strong>欧几里得径向基单元（Euclidean RBF unit）</strong>组成，其本质是一种对输入特征和目标特征之间差异程度的度量。与之相对应的，LeNet-5的原始论文中使用了一种经过改进的<strong>均方误差（Mean Squared Error）</strong>函数作为整个训练时的损失函数。随着后来深度学习模型的发展我们知道，这种度量方法逐渐被<strong>Softmax层+交叉熵损失（Cross Entropy Loss）</strong>的方法所取代。由于手动实现原始论文中的这一特殊单元和损失函数较为繁琐，且这并不是本文的重点，因此这里我们统一使用Softmax层+交叉熵损失的方法来实现分类结构。</p>
<p>我们使用<strong>TorchInfo</strong>工具打印出我们实现的LeNet-5网络结构，结果如下图所示。可以看到，整个网络共有<strong>61750</strong>个可训练参数，并且除了S3层和最后的输出层，其他网络层的参数和原始论文中相一致，表明了我们实现的正确性。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/lenet_implement_summary.png" srcset="/img/loading.gif" lazyload class="" title="lenet_implement_summary">
<h3 id="alexnet">AlexNet</h3>
<h4 id="模型介绍-1">模型介绍</h4>
<p><strong>AlexNet</strong>是由<strong>Alex Krizhevsky</strong>和<strong>Geoffrey Hinton</strong>等人于2012年提出的一种深度卷积神经网络构架，正是这一构架的提出使得深度学习方法在图像识别领域重新获得了新生。</p>
<p>AlexNet的整体网络构架如下图所示（Figure ）。事实上，通过阅读论文我们可以发现，这张原始论文中的构架图带有一定的误导性。由于在当时显存和算力并不足以对整个网络进行存储和计算，因此作者使用了<strong>并行计算</strong>的方法将整个网络拆成了两个部分同时进行计算。对于抽象的网络结构，我们需要将图中的上下两部分进行合并，也即<strong>每一层的通道数实际上应该为图中每一部分所展示的两倍</strong>。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/alexnet_architecture.png" srcset="/img/loading.gif" lazyload class="" title="alexnet_architecture">
<p>可以看到，整个AlexNet网络共有<strong>11</strong>层。与LeNet类似，AlexNet同样由卷积层、池化层和全连接层组成。其中，第一卷积层拥有<strong>96</strong>个卷积核，每个卷积核大小为 <span class="math inline">\(11 \times 11\)</span>，卷积步长为<strong>4</strong>；第二卷积层有<strong>256</strong>个卷积核，每个卷积核大小为 <span class="math inline">\(5 \times 5\)</span>，边界填充大小为<strong>2</strong>；第三卷积层由3个卷积核大小为3的卷积层组成，分别拥有<strong>384</strong>、<strong>384</strong>和<strong>256</strong>个卷积核。在每个卷积层后，均有一个池化核大小为 <span class="math inline">\(3 \times 3\)</span> 且步长为<strong>2</strong>的池化层。整个网络的最后由3个全连接层组成，每层分别有<strong>4096</strong>、<strong>4096</strong>和<strong>10</strong>个单元，这些全连接层将作为最终的特征分类器得到分类的结果。相比LeNet，AlexNet使用了<strong>更大的卷积核</strong>和<strong>更多的卷积核全连接结构</strong>，从而使得整个网络拥有了更好的泛化能力。</p>
<p>对于非线性层，AlexNet首次提出了<strong>ReLU（Rectified Linear Units）</strong>激活函数，其定义为： <span class="math display">\[
\textrm{ReLU}(x) = \max \{0, x\}
\]</span>。 相比较LeNet使用的<strong>sigmoid</strong>和<strong>tanh</strong>激活函数（在AlexNet的原始论文中将其称为饱和激活函数），ReLU拥有更大的梯度，在训练过程中可以有效防止梯度过小和梯度消失等问题，从而加快网络的收敛。这一点在使用类梯度下降作为网络参数优化算法的年代是十分重要的。</p>
<p>此外，AlexNet在最后的全连接层还使用了<strong>Dropout</strong>方法来防止网络过拟合，这一方法也是同年由Krizhevsky和Hinton等人所提出的。Dropout方法的基本原理如下图所示。由于样本量有限，为了防止整个网络参数过拟合，我们在训练时随机关闭一些隐藏层单元使其不参与传播，这样每次参与传播的参数就仅为网络的一部分，且由于随机性，这样做就能有效缓解整个网络过拟合的问题。每次随机关闭隐藏单元的概率被称为<strong>丢弃率</strong>，是整个网络的超参数之一，通常设为 <span class="math inline">\(0.5\)</span> 左右。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/dropout_illustrate.png" srcset="/img/loading.gif" lazyload class="" title="dropout_illustrate">
<h4 id="模型实现-1">模型实现</h4>
<p>由于AlexNet原本是基于<strong>ImageNet</strong>数据集设计的（图像尺寸为 <span class="math inline">\(3 \times 224 \times 224\)</span>），而MNIST数据集中的原始图像尺寸为 <span class="math inline">\(1 \times 28 \times 28\)</span>， 这一输入在后几层卷积层中甚至无法完成一次卷积操作，因此我们首先需要将其放缩为与ImageNet中图像一致的 <span class="math inline">\(224 \times 224\)</span> 大小。在Pytorch中，我们只需要在网络的输入层后增加一个<strong>上采样（Upsample）</strong>层即可。对于当前数据集，我们设置缩放系数为<strong>8</strong>，上采样方法为<strong>双线性差值法（Bilinear）</strong>。</p>
<p>对于网络的其他部分，我们只需要根据上文中的网络结构一一添加相应层即可。完整实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AlexNet</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, dropout</span>):</span><br>        super(AlexNet, self).__init__()<br>        self.net = nn.Sequential(<br>            nn.Upsample(scale_factor=<span class="hljs-number">8</span>, mode=<span class="hljs-string">&#x27;bilinear&#x27;</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">96</span>, out_channels=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>            nn.Conv2d(in_channels=<span class="hljs-number">256</span>, out_channels=<span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(),<br>            nn.Conv2d(in_channels=<span class="hljs-number">384</span>, out_channels=<span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(),<br>            nn.Conv2d(in_channels=<span class="hljs-number">384</span>, out_channels=<span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(in_features=<span class="hljs-number">6400</span>, out_features=<span class="hljs-number">4096</span>),<br>            nn.ReLU(),<br>            nn.Dropout(p=dropout),<br>            nn.Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">4096</span>),<br>            nn.ReLU(),<br>            nn.Dropout(p=dropout),<br>            nn.Linear(in_features=<span class="hljs-number">4096</span>, out_features=<span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">return</span> self.net(x)<br></code></pre></td></tr></table></figure>
<p>我们使用TorchInfo打印出AlexNet的网络结构，结果如下图所示。可以看到，整个AlexNet有着<strong>46764746</strong>个可训练参数，这一参数量几乎是LeNet-5的<strong>758</strong>倍，这也印证了AlexNet原始论文中的核心思想，即更大更深的神经网络可以带来更好的图像特征提取能力。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/alexnet_implement_summary.png" srcset="/img/loading.gif" lazyload class="" title="alexnet_implement_summary">
<h3 id="vggnet">VGGNet</h3>
<h4 id="模型介绍-2">模型介绍</h4>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/vgg_structure.png" srcset="/img/loading.gif" lazyload class="" title="vgg_structure">
<h4 id="模型实现-2">模型实现</h4>
<h3 id="resnet">ResNet</h3>
<h4 id="模型介绍-3">模型介绍</h4>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/resnet_structure.png" srcset="/img/loading.gif" lazyload class="" title="resnet_structure">
<h4 id="模型实现-3">模型实现</h4>
<h3 id="mobilenet">MobileNet</h3>
<h4 id="模型介绍-4">模型介绍</h4>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/mobilenet_architecture.png" srcset="/img/loading.gif" lazyload class="" title="mobilenet_architecture">
<h4 id="模型实现-4">模型实现</h4>
<h3 id="convnext">ConvNeXt</h3>
<h4 id="模型介绍-5">模型介绍</h4>
<p>上面所介绍的一系列模型均为CNN热潮兴起后所提出的一系列网络变种，这些改进模型在各种数据集上一度得到了极好的表现。而近几年来，随着注意力机制和预训练模型的兴起，许多基于构架的图像分类网络也随之出现，其中的不少表现都远远优于基于纯CNN构架的网络。是2022年由提出的一种新一代纯CNN图像特征提取构架，且其优秀的表现重新将CNN结构带回了人们的视野中。</p>
<p>ConvNeXt是一个集大成的逐步演化结果，其从ResNet网络构架出发，通过引入Swin Transformer的相关构架设计思想，一步步地将整个网络结构优化到分类效果最好的状态。具体演化过程及其在ImageNet数据集上的表现如下图所示（Figure ）：</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/convnext_gradual_optimize.png" srcset="/img/loading.gif" lazyload class="" title="convnext_gradual_optimize">
<p>可以看到，ConvNeXt引入的主要特性包括了、，，、以及，这些特性均在先前的各种网络结构中被证明是有效的。与其他较为现代的网络结构一样，ConvNeXt同样采用了网络结构块的形式对整个网络结构进行构建，使其可以成规模地扩张到不同级别的任务上。</p>
<p>一个ConvNeXt网络块的基本结构如下图所示（Figure ）。可以看到，相比传统的残差块，ConvNeXt使用了更大的输入层卷积核，将ReLU和Batch Normalization分别替换成了GELU和Layer Normalization，并减少了激活函数的数量。这些修改均借鉴与Swin-Transformer构架的设计，使得分类效果得以获得显著的提升。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/convnext_block_illustration.png" srcset="/img/loading.gif" lazyload class="" title="convnext_block_illustration">
<h4 id="模型实现-5">模型实现</h4>
<p>由于ConvNeXt的网络构架较为复杂，部分子组件的实现参考了其原始代码。首先我们来实现ConvNeXt块中需要用到的层和层。具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">drop_path</span>(<span class="hljs-params">x, drop_prob: float = <span class="hljs-number">0.</span>, training: bool = False</span>):</span><br>    <span class="hljs-keyword">if</span> drop_prob == <span class="hljs-number">0.</span> <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> training:<br>        <span class="hljs-keyword">return</span> x<br>    keep_prob = <span class="hljs-number">1</span> - drop_prob<br>    shape = (x.shape[<span class="hljs-number">0</span>],) + (<span class="hljs-number">1</span>,) * (x.ndim - <span class="hljs-number">1</span>)<br>    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)<br>    random_tensor.floor_()<br>    output = x.div(keep_prob) * random_tensor<br>    <span class="hljs-keyword">return</span> output<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DropPath</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, drop_prob=None</span>):</span><br>        super(DropPath, self).__init__()<br>        self.drop_prob = drop_prob<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">return</span> drop_path(x, self.drop_prob, self.training)<br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LayerNorm</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, normalized_shape, eps=<span class="hljs-number">1e-6</span>, data_format=<span class="hljs-string">&quot;channels_last&quot;</span></span>):</span><br>        super().__init__()<br>        self.weight = nn.Parameter(torch.ones(normalized_shape), requires_grad=<span class="hljs-literal">True</span>)<br>        self.bias = nn.Parameter(torch.zeros(normalized_shape), requires_grad=<span class="hljs-literal">True</span>)<br>        self.eps = eps<br>        self.data_format = data_format<br>        <span class="hljs-keyword">if</span> self.data_format <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;channels_last&quot;</span>, <span class="hljs-string">&quot;channels_first&quot;</span>]:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;not support data format &#x27;<span class="hljs-subst">&#123;self.data_format&#125;</span>&#x27;&quot;</span>)<br>        self.normalized_shape = (normalized_shape,)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-keyword">if</span> self.data_format == <span class="hljs-string">&quot;channels_last&quot;</span>:<br>            <span class="hljs-keyword">return</span> F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)<br>        <span class="hljs-keyword">elif</span> self.data_format == <span class="hljs-string">&quot;channels_first&quot;</span>:<br>            mean = x.mean(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            var = (x - mean).pow(<span class="hljs-number">2</span>).mean(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>            x = (x - mean) / torch.sqrt(var + self.eps)<br>            x = self.weight[:, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>] * x + self.bias[:, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>]<br>            <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<p>有了上面的组件后，我们来构建。为了配合上面实现的LayerNorm层的输入格式，我们需要在第一个卷积层后将数据的排列方式进行变换，而后续的三层由于不涉及到通道的问题，因此可以直接进行传播。最后，我们将数据变换回原先的排布方式，并对输出进行残差连接和Dropout操作。实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Block</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, channels, drop_rate=<span class="hljs-number">0.</span>, layer_scale_init_value=<span class="hljs-number">1e-6</span></span>):</span><br>        super().__init__()<br>        self.dwconv = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=<span class="hljs-number">7</span>, padding=<span class="hljs-number">3</span>, groups=channels)<br>        self.body = nn.Sequential(<br>            LayerNorm(channels, eps=<span class="hljs-number">1e-6</span>, data_format=<span class="hljs-string">&quot;channels_last&quot;</span>),<br>            nn.Linear(in_features=channels, out_features=<span class="hljs-number">4</span> * channels),<br>            nn.GELU(),<br>            nn.Linear(in_features=<span class="hljs-number">4</span> * channels, out_features=channels)<br>        )<br>        <span class="hljs-keyword">if</span> layer_scale_init_value &gt; <span class="hljs-number">0</span>:<br>            self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((channels,)), requires_grad=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">else</span>:<br>            self.gamma = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> drop_rate &gt; <span class="hljs-number">0.</span>:<br>            self.drop_path = DropPath(drop_rate)<br>        <span class="hljs-keyword">else</span>:<br>            self.drop_path = nn.Identity()<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        y = self.dwconv(x)<br>        <span class="hljs-comment"># [N, C, H, W] -&gt; [N, H, W, C]</span><br>        y = y.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>        y = self.body(y)<br>        <span class="hljs-keyword">if</span> self.gamma <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            y = self.gamma * y<br>        <span class="hljs-comment"># [N, H, W, C] -&gt; [N, C, H, W]</span><br>        y = y.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        out = x + self.drop_path(y)<br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure>
<p>随后我们就可以来实现整个ConvNeXt网络结构了。与前面几种网络结构类似，ConvNeXt依旧遵循单一块的不同变种组成整个卷积部分的模式。由于数据集的单一性，我们在这里仅实现规模较小的ConvNeXt-S网络即可满足分类的模型容量要求。完整实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConvNeXT</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, drop_path_rate: float = <span class="hljs-number">0.</span>, layer_scale_init_value: float = <span class="hljs-number">1e-6</span>,</span></span><br><span class="hljs-function"><span class="hljs-params">                 head_init_scale: float = <span class="hljs-number">1.</span></span>):</span><br>        super().__init__()<br>        depths = [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">27</span>, <span class="hljs-number">3</span>]<br>        dims = [<span class="hljs-number">96</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">768</span>]<br>        self.up = nn.Upsample(scale_factor=<span class="hljs-number">8</span>, mode=<span class="hljs-string">&#x27;bilinear&#x27;</span>)<br>        self.downsample_layers = nn.ModuleList()<br>        stem = nn.Sequential(<br>            nn.Conv2d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">4</span>, stride=<span class="hljs-number">4</span>),<br>            LayerNorm(dims[<span class="hljs-number">0</span>], eps=<span class="hljs-number">1e-6</span>, data_format=<span class="hljs-string">&quot;channels_first&quot;</span>)<br>        )<br>        self.downsample_layers.append(stem)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>):<br>            downsample_layer = nn.Sequential(LayerNorm(dims[i], eps=<span class="hljs-number">1e-6</span>, data_format=<span class="hljs-string">&quot;channels_first&quot;</span>),<br>                                             nn.Conv2d(dims[i], dims[i+<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>            self.downsample_layers.append(downsample_layer)<br><br>        self.stages = nn.ModuleList()<br>        dp_rates = [x.item() <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> torch.linspace(<span class="hljs-number">0</span>, drop_path_rate, sum(depths))]<br>        cur = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>):<br>            stage = nn.Sequential(<br>                *[Block(channels=dims[i], drop_rate=dp_rates[cur + j], layer_scale_init_value=layer_scale_init_value)<br>                  <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(depths[i])]<br>            )<br>            self.stages.append(stage)<br>            cur += depths[i]<br>        self.norm = nn.LayerNorm(dims[<span class="hljs-number">-1</span>], eps=<span class="hljs-number">1e-6</span>)  <span class="hljs-comment"># final norm layer</span><br>        self.head = nn.Linear(dims[<span class="hljs-number">-1</span>], <span class="hljs-number">10</span>)<br>        self.apply(self._init_weights)<br>        self.head.weight.data.mul_(head_init_scale)<br>        self.head.bias.data.mul_(head_init_scale)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_init_weights</span>(<span class="hljs-params">self, m</span>):</span><br>        <span class="hljs-keyword">if</span> isinstance(m, (nn.Conv2d, nn.Linear)):<br>            nn.init.trunc_normal_(m.weight, std=<span class="hljs-number">0.2</span>)<br>            nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward_features</span>(<span class="hljs-params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>):<br>            x = self.downsample_layers[i](x)<br>            x = self.stages[i](x)<br>        <span class="hljs-keyword">return</span> self.norm(x.mean([<span class="hljs-number">-2</span>, <span class="hljs-number">-1</span>]))<br><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br>        x = self.up(x)<br>        x = self.forward_features(x)<br>        x = self.head(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>
<p>我们使用TorchInfo打印出ConvNeXt的网络结构，结果如下图所示（Figure ）。可以看到，作为一个2020年代的卷积神经网络，即使是规模较小的ConvNeXt-S网络的可训练参数依然高达4900万。并且由于大量的中间推理过程，训练和推理这一网络需要一个性能较为强劲的平台来作支撑。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/convnext_implement_summary.png" srcset="/img/loading.gif" lazyload class="" title="convnext_implement_summary">
<h2 id="效果对比">效果对比</h2>
<h3 id="分类效果">分类效果</h3>
<p>我们使用 MNIST 数据集中的训练集对上面实现的所有网络进行了训练，并使用训练好的网络在测试集上进行了分类预测。对于所有训练过程，我们统一使用<strong>交叉熵损失（Cross Entropy Loss）</strong>作为训练时的损失函数，并使用<strong>AdamW</strong>作为优化器。为了横向比较网络结构对分类结果的影响，我们在训练过程中使用了完全相同的超参数（<span class="math inline">\(Batch\_Size = 64, lr = 0.001, Dropout = 0.5, epoch = 5\)</span>）。对于分类结果的评价指标，我们使用了<strong>正确率（Accuracy）</strong>、<strong>准确率（Precision）</strong>、 <strong>召回率（Recall）</strong>和 <strong>F1值</strong>，其定义如下： <span class="math display">\[
\begin{aligned}
&amp;\textrm{Accuracy} = \frac{n_{correct}}{n_{total}}
&amp;\textrm{Precision} = \frac{TP}{TP+FP} \\
&amp;\textrm{Recall} = \frac{TP}{TP+FN}
&amp;\textrm{F1} = \frac{2 \times \textrm{Precision} \times \textrm{Recall}}{\textrm{Precision} + \textrm{Recall}}
\end{aligned}
\]</span> 其中 <span class="math inline">\(TP\)</span>、<span class="math inline">\(FP\)</span>、<span class="math inline">\(FN\)</span> 分别为<strong>正确正例</strong>、<strong>错误正例</strong>和<strong>错误负例</strong>。由于当前问题为多分类问题，我们采用<strong>宏平均（Macro Average）</strong>的方式来计算多个类的平均指标结果。</p>
<p>各个模型的分类结果如下表所示：</p>
<table>
<thead>
<tr class="header">
<th>模型名</th>
<th>正确率</th>
<th>精确率</th>
<th>召回率</th>
<th>F1值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LeNet</td>
<td>0.9744</td>
<td>0.9748</td>
<td>0.9740</td>
<td>0.9743</td>
</tr>
<tr class="even">
<td>AlexNet</td>
<td>0.9869</td>
<td>0.9868</td>
<td>0.9868</td>
<td>0.9867</td>
</tr>
<tr class="odd">
<td>VGGNet-11</td>
<td>0.9828</td>
<td>0.9827</td>
<td>0.9827</td>
<td>0.9827</td>
</tr>
<tr class="even">
<td>ResNet-18</td>
<td><strong>0.9928</strong></td>
<td><strong>0.9928</strong></td>
<td><strong>0.9928</strong></td>
<td><strong>0.9928</strong></td>
</tr>
<tr class="odd">
<td>MobileNet</td>
<td>0.9919</td>
<td>0.9919</td>
<td>0.9918</td>
<td>0.9919</td>
</tr>
<tr class="even">
<td>ConvNeXt</td>
<td>0.9871</td>
<td>0.9871</td>
<td>0.9869</td>
<td>0.9869</td>
</tr>
</tbody>
</table>
<p>可以看到，<strong>ResNet-18</strong>在所有模型中获得了最好的分类效果，MobileNet和ConvNeXt的分类效果其次。提出时间较早的LeNet、AlexNet和VGGNet-11的分类效果排在了最后。为了保证结果的<strong>可复现性</strong>，我们使用<strong>torch.manual_seed()</strong>方法对参数的初始化随机种子进行了固定（随机种子为214748364）。</p>
<p>接下来我们对训练过程中各个模型的收敛情况进行跟踪。各个模型在训练过程中的Loss变化及每个Epoch后在测试集上的分类变化情况如下图所示。可以看到，由于随机梯度下降方法的特征随机性，同一模型在不同Batch中得到的Loss呈波动状态，但总体随着迭代步数（Step）的增加逐渐下降。从分类结果来看，ResNet和MobileNet的收敛速度极快，且始终保持在一个较高的水平逐步提升；AlexNet和VGGNet由于其模型容量的限制，其准确率在达到一定的高度后便很难再出现显著提升；LeNet和ConvNeXt的准确率变化幅度较大，并且ConvNeXt在五次迭代后分类准确率并没有趋近平缓，表明此时模型还没有完全收敛。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/model_loss_accuracy.png" srcset="/img/loading.gif" lazyload class="" title="model_loss_accuracy">
<p>接下来，我们使用不同的优化器对各个模型进行训练，以衡量相同条件下不同模型训练的困难程度。这里我们分别使用了<strong>SGD</strong>、<strong>Adam</strong>和<strong>AdamW</strong>三种优化器，对应于三种参数自适应能力递增的优化算法。对于SGD优化器，我们将学习率固定在<strong>0.1</strong>；对于Adam和AdamW优化器，我们将初始学习率设为<strong>0.001</strong>。各个模型在不同优化器下训练时的Loss变化情况如下图所示。可以看到，SGD优化器在<strong>LeNet-5</strong>、<strong>VGGNet-11</strong>和<strong>ConvNeXT</strong>上表现不佳，其中后两者的Loss在下降到一定程度后便不再下降，表明优化失败。而在剩下的三个模型上，SGD优化器则获得了较好的表现，模型参数逐渐收敛到了预期的效果。其他两个优化器由于拥有较强的适应能力，因此其在各个模型上均能取得较好的表现。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/optimizer_loss.png" srcset="/img/loading.gif" lazyload class="" title="optimizer_loss">
<p>从上面的分析我们可以知道，<strong>ResNet-18</strong>和<strong>MobileNet</strong>在实际应用场景下相对容易训练，而剩下的模型则需要较为强劲的优化器才能够获得较好的表现，这也从一定程度上解释了这两大模型流行的原因。</p>
<h3 id="效能">效能</h3>
<p>接下来我们对各个模型的计算资源效能进行评估。在当前实验下，我们使用<strong>Intel Xeon Gold 5318Y + NVIDIA A40</strong>平台进行了全部的训练和推理。对于训练过程，我们统一使用<strong>FP32</strong>作为计算精度，Batch Size 统一为 <strong>64</strong>，并将训练过程中的模型和中间数据全部放在<strong>CUDA缓存</strong>中。在这一配置下，各个模型在训练时的显存占用情况如下图所示。可以看到，较早提出的 LeNet 和 AlexNet 所占用的显存空间较小，能够在各种性能等级的平台上完成推理；ResNet 和 MobileNet 由于较为巧妙的结构设计，将显存占用量控制在了一个较为合适的水平内;而依赖深度堆叠的 VGGNet 和较新提出的 ConvNeXt 模型则需要较多的显存资源才能够完成推理（通过参考基线可以看出要完成 VGGNet 的训练至少需要一张 RTX 3080Ti 或同级别的显卡，而完 成 ConvNeXT 的训练则这少需要一张 RTX 3090 或同级别的显卡）。</p>
<img src="/2022/05/02/contemporary-ai/contemporary-ai-exp-3/cuda_usage.png" srcset="/img/loading.gif" lazyload class="" title="cuda_usage">
<p>对于计算资源的占用情况，我们通过<strong>训练用时</strong>、<strong>推理用时</strong>和<strong>浮点运算数（Floating Point Operations, FLOPs）</strong>三种指标来进行评估。其中，第三个指标衡量了<strong>模型完成一次推理所需要进行的运算量</strong>，是模型复杂程度的一个量化指标。在具体实现中，我们使用<strong>ptflops</strong>包中提供的<strong>get_model_complexity_info()</strong>函数来计算各个模型的 FLOPs 值，并使用<strong>time</strong>包中的<strong>perf_counter()</strong>函数计算其资源占用时间。计算结果如下表所示。可以看到，较早的模型往往拥有较小的模型复杂度和较快的推理时间，而较为现代的模型则通常拥有千万级甚至亿级的模型复杂度，且需要更为漫长的推理和训练时间。对于 2022 年最新的 ConvNext 模型，即使是其较小参数量的 Small 版本，在 Nvidia A40 服务级显卡上进行 5 次迭代训练也要<strong>超过 1660 秒（接近半小时）</strong>的训练时间，这一资源需求显然是桌面级设备所不能接受的。</p>
<table>
<thead>
<tr class="header">
<th>模型名</th>
<th>FLOPs</th>
<th>训练用时（秒）</th>
<th>推理用时（秒）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LeNet</td>
<td>420K</td>
<td>56.6359</td>
<td>0.0317</td>
</tr>
<tr class="even">
<td>AlexNet</td>
<td>940M</td>
<td>101.1776</td>
<td>0.0349</td>
</tr>
<tr class="odd">
<td>VGGNet-11</td>
<td>7.57G</td>
<td>653.7334</td>
<td>0.0703</td>
</tr>
<tr class="even">
<td>ResNet-18</td>
<td>1.75G</td>
<td>277.5996</td>
<td>0.0450</td>
</tr>
<tr class="odd">
<td>MobileNet</td>
<td>580M</td>
<td>288.4978</td>
<td>0.0512</td>
</tr>
<tr class="even">
<td>ConvNeXt-S</td>
<td>8.69G</td>
<td>1665.7066</td>
<td>0.1167</td>
</tr>
</tbody>
</table>
<h2 id="总结">总结</h2>
<p>在本实验中，我们分析并实现了基于卷积神经网络的 6 大图像分类网络构架:LeNet、AlexNet、VGGNet、ResNet、MobileNet 和 ConvNeXt，涵盖了自 1998 以来计算机视觉领域中的大部分 要网络结构和思想。通过在多个维度上比较不同网络构架在 MNIST 数据集上的训练和推理表现，我们能够较为全面的了解不同网络构架的特性和适用场景。在实现的过程中，我们能够看到深度学习方法在图像分类和图像模式识别任务上是如何一步步发展到今天的，也看到了计算资源的发展对模型结构的扩展起到的重要作用。</p>
<h2 id="references">References</h2>
<ol type="1">
<li>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.</li>
<li>Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus), 2016.</li>
<li>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. ArXiv, abs/1207.0580, 2012.</li>
<li>Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Eﬀicient convolutional neural networks for mobile vision applications. ArXiv, abs/1704.04861, 2017.</li>
<li>Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.</li>
<li>Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML’15, page 448–456. JMLR.org, 2015.</li>
<li>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.</li>
<li>Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.</li>
<li>Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation, 1(4):541–551, 12 1989.</li>
<li>Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li>
<li>Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation. In Proceedings of the 31st International Conference on Neural In- formation Processing Systems, NIPS’17, page 597–607, Red Hook, NY, USA, 2017. Curran Associates Inc.</li>
<li>Cory Maklin. Dropout neural network layer in keras explained. https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab, 2019.</li>
<li>Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4510–4520, 2018.</li>
<li>K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, May 2015.</li>
<li>TylerYep. torchinfo. https://github.com/TylerYep/torchinfo, 2020.</li>
<li>Saining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5987–5995, 2017.</li>
<li>Christopher J.C. Burges Yann LeCun, Corinna Cortes. The mnist database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 1998.</li>
<li>Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning. arXiv preprint arXiv:2106.11342, 2021.</li>
</ol>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E5%BD%93%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">当代人工智能</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Computer-Science/">Computer-Science</a>
                    
                      <a class="hover-with-bg" href="/tags/Artificial-Intelligence/">Artificial-Intelligence</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/06/30/contemporary-ai/contemporary-ai-exp-4/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">当代人工智能 课程项目四 预训练模型的加载与使用（Transformers）实验报告</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/04/05/contemporary-ai/contemporary-ai-exp-2/">
                        <span class="hidden-mobile">当代人工智能 课程项目二 A*算法</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
